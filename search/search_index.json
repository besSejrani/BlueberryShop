{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Introduction TPI Goals Shopping Cart List all products in the cart Each product in the cart can be deleted Calculate the total cart Data submission & persistance The data is send to the server, error handling is applied The order data is stored in the database Payment handling with Stripe The Stripe API is used for handling payments Order History The order history is showed with date and cart total The order details are showed Server-side rendered order PDF The PDF must be generated on the server","title":"Introduction"},{"location":"index.html#introduction","text":"","title":"Introduction"},{"location":"index.html#tpi-goals","text":"","title":"TPI Goals"},{"location":"index.html#shopping-cart","text":"List all products in the cart Each product in the cart can be deleted Calculate the total cart","title":"Shopping Cart"},{"location":"index.html#data-submission-persistance","text":"The data is send to the server, error handling is applied The order data is stored in the database","title":"Data submission &amp; persistance"},{"location":"index.html#payment-handling-with-stripe","text":"The Stripe API is used for handling payments","title":"Payment handling with Stripe"},{"location":"index.html#order-history","text":"The order history is showed with date and cart total The order details are showed","title":"Order History"},{"location":"index.html#server-side-rendered-order-pdf","text":"The PDF must be generated on the server","title":"Server-side rendered order PDF"},{"location":"1.Installation/0.Project.html","text":"Blueberry Project Two Installation BlueberryShop can be installed in two different ways, via a manual installation or via containers. Download Project Git git clone git@github.com:besSejrani/BlueberryShop.git Manual Installation Prerequisites Theses are the prerequesites for manually installing BlueberryShop. Install Make Install MongoDB Install Redis Install Mkdocs Install Nodejs Install Nginx Container Installation Prerequisites Theses are the prerequesites for installing BlueberryShop via containers. Install Make Install Docker / Docker Desktop Install Docker-compose","title":"Blueberry Project"},{"location":"1.Installation/0.Project.html#blueberry-project","text":"","title":"Blueberry Project"},{"location":"1.Installation/0.Project.html#two-installation","text":"BlueberryShop can be installed in two different ways, via a manual installation or via containers.","title":"Two Installation"},{"location":"1.Installation/0.Project.html#download-project","text":"Git git clone git@github.com:besSejrani/BlueberryShop.git","title":"Download Project"},{"location":"1.Installation/0.Project.html#manual-installation-prerequisites","text":"Theses are the prerequesites for manually installing BlueberryShop. Install Make Install MongoDB Install Redis Install Mkdocs Install Nodejs Install Nginx","title":"Manual Installation Prerequisites"},{"location":"1.Installation/0.Project.html#container-installation-prerequisites","text":"Theses are the prerequesites for installing BlueberryShop via containers. Install Make Install Docker / Docker Desktop Install Docker-compose","title":"Container Installation Prerequisites"},{"location":"1.Installation/Containers/0.Make.html","text":"Make Use Case Make allows to automate a program's build process. By installing the project via containers, make is mostly used to inject a environment variable to Docker-Compose which will then be injected in the application. Makefile # Production run-prod : ENVIRONMENT = production docker-compose up --build # Development run-dev : ENVIRONMENT = development docker-compose up --build # Test run-test : ENVIRONMENT = test docker-compose up --build Installation Windows Make for Windows can be installed via the following link . Don't forget to add Make's path to your environment variables if the installer doesn't do it automatically. Brew brew install make Linux # Update & upgrade packages sudo apt update sudo apt upgrade # Install Make sudo apt install make Containers List Containers docker-compose ps Stop Containers docker-compose down Delete All Docker Containers & Volumes docker system prune Running Services Service Port Nginx 8443 Client 3000 Server 4000 Mkdocs 9000 Redis 6379 RedisInsight 8001 MongoDB 27017 Sources Source Author Link Make for Windows Sourceforge Link","title":"Make"},{"location":"1.Installation/Containers/0.Make.html#make","text":"","title":"Make"},{"location":"1.Installation/Containers/0.Make.html#use-case","text":"Make allows to automate a program's build process. By installing the project via containers, make is mostly used to inject a environment variable to Docker-Compose which will then be injected in the application. Makefile # Production run-prod : ENVIRONMENT = production docker-compose up --build # Development run-dev : ENVIRONMENT = development docker-compose up --build # Test run-test : ENVIRONMENT = test docker-compose up --build","title":"Use Case"},{"location":"1.Installation/Containers/0.Make.html#installation","text":"Windows Make for Windows can be installed via the following link . Don't forget to add Make's path to your environment variables if the installer doesn't do it automatically. Brew brew install make Linux # Update & upgrade packages sudo apt update sudo apt upgrade # Install Make sudo apt install make","title":"Installation"},{"location":"1.Installation/Containers/0.Make.html#containers","text":"List Containers docker-compose ps Stop Containers docker-compose down Delete All Docker Containers & Volumes docker system prune","title":"Containers"},{"location":"1.Installation/Containers/0.Make.html#running-services","text":"Service Port Nginx 8443 Client 3000 Server 4000 Mkdocs 9000 Redis 6379 RedisInsight 8001 MongoDB 27017","title":"Running Services"},{"location":"1.Installation/Containers/0.Make.html#sources","text":"Source Author Link Make for Windows Sourceforge Link","title":"Sources"},{"location":"1.Installation/Containers/1.Docker.html","text":"Docker Installation This project relies on Docker, please make sure it is installed on your system, documentation can be found here . Configuration Dockerfile Example FROM node:alpine as builder WORKDIR /app COPY package*.json . RUN npm install --force COPY . . EXPOSE 3000 CMD [\"npm\", \"run\", \"dev\"] Sources Source Author Link Install Docker Engine on Ubuntu Docker Link","title":"Docker"},{"location":"1.Installation/Containers/1.Docker.html#docker","text":"","title":"Docker"},{"location":"1.Installation/Containers/1.Docker.html#installation","text":"This project relies on Docker, please make sure it is installed on your system, documentation can be found here .","title":"Installation"},{"location":"1.Installation/Containers/1.Docker.html#configuration","text":"Dockerfile Example FROM node:alpine as builder WORKDIR /app COPY package*.json . RUN npm install --force COPY . . EXPOSE 3000 CMD [\"npm\", \"run\", \"dev\"]","title":"Configuration"},{"location":"1.Installation/Containers/1.Docker.html#sources","text":"Source Author Link Install Docker Engine on Ubuntu Docker Link","title":"Sources"},{"location":"1.Installation/Containers/2.Docker-Compose.html","text":"Docker-Compose Installation For better container management, docker-compose is used in development, make sure as well, that it is installed on your system, documentation can be found here . Configuration For custom configuration, the docker-compose.yaml file can be edited to match your use case, documentation can be found here . docker-compose.yaml Example version : \"3.8\" services : mkdocs : restart : always container_name : mkdocs build : dockerfile : Dockerfile context : ./Documentation ports : - \"9000:8000\" volumes : - ./Documentation:/app nginx : restart : always build : dockerfile : Dockerfile context : ./Nginx container_name : nginx depends_on : - client - server networks : - backend - frontend ports : - \"8000:80\" - \"8443:443\" volumes : - ./Nginx/server.crt:/etc/nginx/server.crt - ./Nginx/server.key:/etc/nginx/server.key client : build : context : ./Client container_name : client networks : - frontend ports : - \"3000:3000\" depends_on : - server env_file : - ./Client/.env volumes : - ./Client:/app server : build : context : ./Server container_name : server networks : - backend - frontend ports : - \"4000:4000\" depends_on : - mongo - redis environment : - NODE_ENV2=${ENVIRONMENT} env_file : - ./Server/.env volumes : - ./Server:/app redis : image : redis:latest # image: redislabs/redismod container_name : redis networks : - backend ports : - \"6379:6379\" command : redis-server # Monitoring GUI redisinsight : image : redislabs/redisinsight:latest container_name : redisinsight ports : - \"8001:8001\" networks : - backend user : root volumes : - ~/docker/BlueberryShop/RedisInsight:/db mongo : container_name : mongodb image : mongo:latest restart : always ports : - 27017:27017 networks : - backend environment : MONGO_INITDB_ROOT_USERNAME : root MONGO_INITDB_ROOT_PASSWORD : password MONGO_INITDB_DATABASE : root-db volumes : - ./Mongo/mongo-init.js:/docker-entrypoint-initdb.d/mongo-init.js:ro - ~/docker/BlueberryShop/Mongo:/data/db # =========================================================================================================================== networks : backend : frontend : Sources Source Author Link Install Docker Compose Docker Link","title":"Docker-Compose"},{"location":"1.Installation/Containers/2.Docker-Compose.html#docker-compose","text":"","title":"Docker-Compose"},{"location":"1.Installation/Containers/2.Docker-Compose.html#installation","text":"For better container management, docker-compose is used in development, make sure as well, that it is installed on your system, documentation can be found here .","title":"Installation"},{"location":"1.Installation/Containers/2.Docker-Compose.html#configuration","text":"For custom configuration, the docker-compose.yaml file can be edited to match your use case, documentation can be found here . docker-compose.yaml Example version : \"3.8\" services : mkdocs : restart : always container_name : mkdocs build : dockerfile : Dockerfile context : ./Documentation ports : - \"9000:8000\" volumes : - ./Documentation:/app nginx : restart : always build : dockerfile : Dockerfile context : ./Nginx container_name : nginx depends_on : - client - server networks : - backend - frontend ports : - \"8000:80\" - \"8443:443\" volumes : - ./Nginx/server.crt:/etc/nginx/server.crt - ./Nginx/server.key:/etc/nginx/server.key client : build : context : ./Client container_name : client networks : - frontend ports : - \"3000:3000\" depends_on : - server env_file : - ./Client/.env volumes : - ./Client:/app server : build : context : ./Server container_name : server networks : - backend - frontend ports : - \"4000:4000\" depends_on : - mongo - redis environment : - NODE_ENV2=${ENVIRONMENT} env_file : - ./Server/.env volumes : - ./Server:/app redis : image : redis:latest # image: redislabs/redismod container_name : redis networks : - backend ports : - \"6379:6379\" command : redis-server # Monitoring GUI redisinsight : image : redislabs/redisinsight:latest container_name : redisinsight ports : - \"8001:8001\" networks : - backend user : root volumes : - ~/docker/BlueberryShop/RedisInsight:/db mongo : container_name : mongodb image : mongo:latest restart : always ports : - 27017:27017 networks : - backend environment : MONGO_INITDB_ROOT_USERNAME : root MONGO_INITDB_ROOT_PASSWORD : password MONGO_INITDB_DATABASE : root-db volumes : - ./Mongo/mongo-init.js:/docker-entrypoint-initdb.d/mongo-init.js:ro - ~/docker/BlueberryShop/Mongo:/data/db # =========================================================================================================================== networks : backend : frontend :","title":"Configuration"},{"location":"1.Installation/Containers/2.Docker-Compose.html#sources","text":"Source Author Link Install Docker Compose Docker Link","title":"Sources"},{"location":"1.Installation/Manual/0.Make.html","text":"Make Use Case Make allows to automate a program's build process. By installing the project manually, make is mostly used for installing software. Installation Windows Make for Windows can be installed via the following link . Don't forget to add Make's path to your environment variables if the installer doesn't do it automatically. Brew brew install make Linux # Update & upgrade packages sudo apt update sudo apt upgrade # Install Make sudo apt install make Sources Source Author Link Make for Windows Sourceforge Link","title":"Make"},{"location":"1.Installation/Manual/0.Make.html#make","text":"","title":"Make"},{"location":"1.Installation/Manual/0.Make.html#use-case","text":"Make allows to automate a program's build process. By installing the project manually, make is mostly used for installing software.","title":"Use Case"},{"location":"1.Installation/Manual/0.Make.html#installation","text":"Windows Make for Windows can be installed via the following link . Don't forget to add Make's path to your environment variables if the installer doesn't do it automatically. Brew brew install make Linux # Update & upgrade packages sudo apt update sudo apt upgrade # Install Make sudo apt install make","title":"Installation"},{"location":"1.Installation/Manual/0.Make.html#sources","text":"Source Author Link Make for Windows Sourceforge Link","title":"Sources"},{"location":"1.Installation/Manual/1.Mongodb.html","text":"MongoDB Use Case BlueberryShop uses MongoDB as a primary database. The great advantage that MongoDB offers as a NoSQL database over traditional SQL databases like MySQL or PostgreSQL is horizontal scaling. MongoDB Installation Windows Don't forget to add MongoDB's path to your environment variables if the installer doesn't do it automatically. Image: MongoDB Installation on Windows MacOS Image: MongoDB Installation on MacOS Linux Image: MongoDB Installation on Ubuntu Linux MongoDB Ubuntu Server Installation # Import public key of the package management system wget -qO - https://www.mongodb.org/static/pgp/server-4.4.asc | sudo apt-key add - # Create the /etc/apt/sources.list.d/mongodb-org-4.4.list file echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/4.4 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.4.list # Reload Packages sudo apt-get update # Install MongoDB sudo apt-get install -y mongodb-org Start Mongod sudo systemctl start mongod Stop Mongod sudo systemctl stop mongod Reload Mongod Daemon sudo systemctl daemon-reload Mongod Status sudo systemctl status mongod Enable Mongod sudo systemctl enable mongod # Run MongoDB mongo MongoDB Database Tools Installation The MongoDB Database Tools are needed for executing the database utilities like mongodump or mongorestore for backup export and import. Since MongoDB version 4.4, the MongoDB Database Tools are now on a separate package Windows Don't forget to add MongoDB's path to your environment variables if the installer doesn't do it automatically. Image: MongoDB Database Tools Installation on Windows MacOS Image: MongoDB Database Tools Installation on MacOS Linux Image: MongoDB Database Tools Installation on Ubuntu Linux MongoDB Atlas MongoDB Atlas is a managed MongoDB database running on the cloud. Including multiple paid tiers, the free tier includes up to 0.5 GB of data. If BlueberryShop should be installed on multiple environments with the same data set, then MongoDB Atlas could be an alternative. MongoDB Compass MongoDB Compass is a GUI application allowing to interact with a MongoDB database, it is comparable to MySQL Workbench or PGAdmin . It is also possible to interact with a MongoDB database via the shell, but for most users, a GUI is most intuitive. Windows Image: MongoDB Compass Installation on Windows MacOS Image: MongoDB Compass Installation on MacOS Linux Image: MongoDB Compass Installation on Ubuntu Database View Image: MongoDB Compass Database View Collection View Image: MongoDB Compass Collection View Running Service Service Port mongod 27017 Sources Source Author Link MongoDB Installation MongoDB Link MongoDB Ubuntu Installation MongoDB Link MongoDB Database Tools Installation MongoDB Link MongoDB Compass Installation MongoDB Link MongoDB Atlas MongoDB Link","title":"MongoDB"},{"location":"1.Installation/Manual/1.Mongodb.html#mongodb","text":"","title":"MongoDB"},{"location":"1.Installation/Manual/1.Mongodb.html#use-case","text":"BlueberryShop uses MongoDB as a primary database. The great advantage that MongoDB offers as a NoSQL database over traditional SQL databases like MySQL or PostgreSQL is horizontal scaling.","title":"Use Case"},{"location":"1.Installation/Manual/1.Mongodb.html#mongodb-installation","text":"Windows Don't forget to add MongoDB's path to your environment variables if the installer doesn't do it automatically. Image: MongoDB Installation on Windows MacOS Image: MongoDB Installation on MacOS Linux Image: MongoDB Installation on Ubuntu Linux","title":"MongoDB Installation"},{"location":"1.Installation/Manual/1.Mongodb.html#mongodb-ubuntu-server-installation","text":"# Import public key of the package management system wget -qO - https://www.mongodb.org/static/pgp/server-4.4.asc | sudo apt-key add - # Create the /etc/apt/sources.list.d/mongodb-org-4.4.list file echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/4.4 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.4.list # Reload Packages sudo apt-get update # Install MongoDB sudo apt-get install -y mongodb-org Start Mongod sudo systemctl start mongod Stop Mongod sudo systemctl stop mongod Reload Mongod Daemon sudo systemctl daemon-reload Mongod Status sudo systemctl status mongod Enable Mongod sudo systemctl enable mongod # Run MongoDB mongo","title":"MongoDB Ubuntu Server Installation"},{"location":"1.Installation/Manual/1.Mongodb.html#mongodb-database-tools-installation","text":"The MongoDB Database Tools are needed for executing the database utilities like mongodump or mongorestore for backup export and import. Since MongoDB version 4.4, the MongoDB Database Tools are now on a separate package Windows Don't forget to add MongoDB's path to your environment variables if the installer doesn't do it automatically. Image: MongoDB Database Tools Installation on Windows MacOS Image: MongoDB Database Tools Installation on MacOS Linux Image: MongoDB Database Tools Installation on Ubuntu Linux","title":"MongoDB Database Tools Installation"},{"location":"1.Installation/Manual/1.Mongodb.html#mongodb-atlas","text":"MongoDB Atlas is a managed MongoDB database running on the cloud. Including multiple paid tiers, the free tier includes up to 0.5 GB of data. If BlueberryShop should be installed on multiple environments with the same data set, then MongoDB Atlas could be an alternative.","title":"MongoDB Atlas"},{"location":"1.Installation/Manual/1.Mongodb.html#mongodb-compass","text":"MongoDB Compass is a GUI application allowing to interact with a MongoDB database, it is comparable to MySQL Workbench or PGAdmin . It is also possible to interact with a MongoDB database via the shell, but for most users, a GUI is most intuitive. Windows Image: MongoDB Compass Installation on Windows MacOS Image: MongoDB Compass Installation on MacOS Linux Image: MongoDB Compass Installation on Ubuntu Database View Image: MongoDB Compass Database View Collection View Image: MongoDB Compass Collection View","title":"MongoDB Compass"},{"location":"1.Installation/Manual/1.Mongodb.html#running-service","text":"Service Port mongod 27017","title":"Running Service"},{"location":"1.Installation/Manual/1.Mongodb.html#sources","text":"Source Author Link MongoDB Installation MongoDB Link MongoDB Ubuntu Installation MongoDB Link MongoDB Database Tools Installation MongoDB Link MongoDB Compass Installation MongoDB Link MongoDB Atlas MongoDB Link","title":"Sources"},{"location":"1.Installation/Manual/2.Redis.html","text":"Redis Use Case BlueberryShop uses Redis as a secondary database. It is mostly used for storing key-value pairs having some Time To Live values. In the future, Redis will be used also for other purposes. Redis Installation Prerequisites Make Redis Installation Windows To install Redis on Windows, first enable Windows Subsystem for Linux, then download Ubuntu or any other Linux distribution, finally proceed with following commands. wget https://download.redis.io/releases/redis-6.2.3.tar.gz tar xzf redis-6.2.3.tar.gz cd redis-6.2.3 make MacOS # Redis needs a C compiler and some utility packages sudo apt install gcc \\ pkg-config \\ tcl-dev wget https://download.redis.io/releases/redis-6.2.3.tar.gz tar xzf redis-6.2.3.tar.gz cd redis-6.2.3 make Ubuntu # Redis needs a C compiler and some utility packages sudo apt install gcc \\ pkg-config \\ tcl-dev wget https://download.redis.io/releases/redis-6.2.3.tar.gz tar xzf redis-6.2.3.tar.gz cd redis-6.2.3 make Run Redis Server redis-server Run Redis CLI redis-cli Heroku Redis Heroku is a Platform As A Service, offering freemium hosting plans. One of it's proposal is a Redis hosting, it's freemium plan limited to 25 MB and the connections are limited to 20. Heroku also rotates credentials periodically and updates applications where this data store is attached. Heroku Redis is great while being in development, but don't use it in production with the freemium plan. RedisInsight Redisinsight is a web application client for Redis, it has as features monitoring, CLI, RedisGraphs, RediSearch and RedisGears. Redisinsight requires a minimum of 8 GB of memory. Windows Image: Redisinsight Windows Installation MacOS Image: Redisinsight MacOS Installation Warning RedisInsight is only supported on Mac hardware with Intel chips. Mac hardware with the Apple M1 (ARM) chip is not supported. Linux Image: Redisinsight Ubuntu Installation Permission chmod +x redisinsight-linux64-<version> Run RedisInsight ./redisinsight-linux64-<version> Monitoring Image: Redis Monitoring CLI Image: Redis CLI Running Service Service Port redis-server 6379 redisinsight 8001 Sources Source Author Link Redis Documentation Redis Link Redis Tutorial Redis University Link Redis Tutorial Programming Skills Link RedisInsight Installation Redislabs Link","title":"Redis"},{"location":"1.Installation/Manual/2.Redis.html#redis","text":"","title":"Redis"},{"location":"1.Installation/Manual/2.Redis.html#use-case","text":"BlueberryShop uses Redis as a secondary database. It is mostly used for storing key-value pairs having some Time To Live values. In the future, Redis will be used also for other purposes.","title":"Use Case"},{"location":"1.Installation/Manual/2.Redis.html#redis-installation-prerequisites","text":"Make","title":"Redis Installation Prerequisites"},{"location":"1.Installation/Manual/2.Redis.html#redis-installation","text":"Windows To install Redis on Windows, first enable Windows Subsystem for Linux, then download Ubuntu or any other Linux distribution, finally proceed with following commands. wget https://download.redis.io/releases/redis-6.2.3.tar.gz tar xzf redis-6.2.3.tar.gz cd redis-6.2.3 make MacOS # Redis needs a C compiler and some utility packages sudo apt install gcc \\ pkg-config \\ tcl-dev wget https://download.redis.io/releases/redis-6.2.3.tar.gz tar xzf redis-6.2.3.tar.gz cd redis-6.2.3 make Ubuntu # Redis needs a C compiler and some utility packages sudo apt install gcc \\ pkg-config \\ tcl-dev wget https://download.redis.io/releases/redis-6.2.3.tar.gz tar xzf redis-6.2.3.tar.gz cd redis-6.2.3 make Run Redis Server redis-server Run Redis CLI redis-cli","title":"Redis Installation"},{"location":"1.Installation/Manual/2.Redis.html#heroku-redis","text":"Heroku is a Platform As A Service, offering freemium hosting plans. One of it's proposal is a Redis hosting, it's freemium plan limited to 25 MB and the connections are limited to 20. Heroku also rotates credentials periodically and updates applications where this data store is attached. Heroku Redis is great while being in development, but don't use it in production with the freemium plan.","title":"Heroku Redis"},{"location":"1.Installation/Manual/2.Redis.html#redisinsight","text":"Redisinsight is a web application client for Redis, it has as features monitoring, CLI, RedisGraphs, RediSearch and RedisGears. Redisinsight requires a minimum of 8 GB of memory. Windows Image: Redisinsight Windows Installation MacOS Image: Redisinsight MacOS Installation Warning RedisInsight is only supported on Mac hardware with Intel chips. Mac hardware with the Apple M1 (ARM) chip is not supported. Linux Image: Redisinsight Ubuntu Installation Permission chmod +x redisinsight-linux64-<version> Run RedisInsight ./redisinsight-linux64-<version> Monitoring Image: Redis Monitoring CLI Image: Redis CLI","title":"RedisInsight"},{"location":"1.Installation/Manual/2.Redis.html#running-service","text":"Service Port redis-server 6379 redisinsight 8001","title":"Running Service"},{"location":"1.Installation/Manual/2.Redis.html#sources","text":"Source Author Link Redis Documentation Redis Link Redis Tutorial Redis University Link Redis Tutorial Programming Skills Link RedisInsight Installation Redislabs Link","title":"Sources"},{"location":"1.Installation/Manual/3.Mkdocs.html","text":"Mkdocs Prerequisites Mkdocs Installation Install Python 3 Install Pip3 Mkdocs Installation Pip3 Package Installation pip3 install mkdocs - material Mkdocs Environment Variable Mkdocs package's path must be set in the path environment variable. This is only needed in Windows. Mkdocs Path Package # Windows Path C :\\ Users \\ USERNAME \\ AppData \\ Local \\ Packages \\ PythonSoftwareFoundation . Python . 3 . 9_qbz5n2kfra8p0 \\ LocalCache \\ local-packages \\ Python39 \\ Scripts \\ Running Documentation Start Server # Change directory to Documentation cd Documentation/ # Start server mkdocs serve Service Port mkdocs 8000 Github Actions Hosting documentation on Github Pages, via Mkdocs, can be done with the help of Github Actions, an automation software. In the following YAML file, each time that code is pushed to the master / main repository branch, it launches a job in an Ubuntu environment and deploys the content to Github Pages. Github Actions name : ci on : push : branches : - master - main jobs : deploy : defaults : run : working-directory : Documentation runs-on : ubuntu-latest steps : - uses : actions/checkout@v2 - uses : actions/setup-python@v2 with : python-version : 3.x - run : pip install mkdocs-material - run : mkdocs gh-deploy --force","title":"Mkdocs"},{"location":"1.Installation/Manual/3.Mkdocs.html#mkdocs","text":"","title":"Mkdocs"},{"location":"1.Installation/Manual/3.Mkdocs.html#prerequisites-mkdocs-installation","text":"Install Python 3 Install Pip3","title":"Prerequisites Mkdocs Installation"},{"location":"1.Installation/Manual/3.Mkdocs.html#mkdocs-installation","text":"Pip3 Package Installation pip3 install mkdocs - material","title":"Mkdocs Installation"},{"location":"1.Installation/Manual/3.Mkdocs.html#mkdocs-environment-variable","text":"Mkdocs package's path must be set in the path environment variable. This is only needed in Windows. Mkdocs Path Package # Windows Path C :\\ Users \\ USERNAME \\ AppData \\ Local \\ Packages \\ PythonSoftwareFoundation . Python . 3 . 9_qbz5n2kfra8p0 \\ LocalCache \\ local-packages \\ Python39 \\ Scripts \\","title":"Mkdocs Environment Variable"},{"location":"1.Installation/Manual/3.Mkdocs.html#running-documentation","text":"Start Server # Change directory to Documentation cd Documentation/ # Start server mkdocs serve Service Port mkdocs 8000","title":"Running Documentation"},{"location":"1.Installation/Manual/3.Mkdocs.html#github-actions","text":"Hosting documentation on Github Pages, via Mkdocs, can be done with the help of Github Actions, an automation software. In the following YAML file, each time that code is pushed to the master / main repository branch, it launches a job in an Ubuntu environment and deploys the content to Github Pages. Github Actions name : ci on : push : branches : - master - main jobs : deploy : defaults : run : working-directory : Documentation runs-on : ubuntu-latest steps : - uses : actions/checkout@v2 - uses : actions/setup-python@v2 with : python-version : 3.x - run : pip install mkdocs-material - run : mkdocs gh-deploy --force","title":"Github Actions"},{"location":"1.Installation/Manual/4.Nodejs.html","text":"Node.js Use Case Node.js is the primary runtime on the server, the heavy use of Input/Output data and it's asynchronous nature makes it a valid choice, despite the fact that it is single threaded. In the future, the server will implement Node.js, Deno and Golang. Install Node.js Windows Don't forget to add Node's path to your environement variables if the installer doesn't do it automatically. Image: Node.js Windows Installation MacOS Image: Node.js MacOS Installation Linux cd ~ curl -sL https://deb.nodesource.com/setup_14.x -o nodesource_setup.sh nano nodesource_setup.sh sudo bash nodesource_setup.sh sudo apt install nodejs node -v Node Version node -v NPM Version NPM is the official Node Package Manager of Node.js, it comes with Node. npm -v Install Project Dependencies Client // NPM cd Client npm install // Yarn cd Client yarn install Server // NPM cd Server npm install // Yarn cd Server yarn install Run Application Script // NPM cd Server npm run dev // Yarn cd Server yarn run dev Running Services Service Port client 3000 server 4000 Sources Source Author Link How To Install Node.js on Ubuntu 20.04 Digital Ocean Link","title":"Node.js"},{"location":"1.Installation/Manual/4.Nodejs.html#nodejs","text":"","title":"Node.js"},{"location":"1.Installation/Manual/4.Nodejs.html#use-case","text":"Node.js is the primary runtime on the server, the heavy use of Input/Output data and it's asynchronous nature makes it a valid choice, despite the fact that it is single threaded. In the future, the server will implement Node.js, Deno and Golang.","title":"Use Case"},{"location":"1.Installation/Manual/4.Nodejs.html#install-nodejs","text":"Windows Don't forget to add Node's path to your environement variables if the installer doesn't do it automatically. Image: Node.js Windows Installation MacOS Image: Node.js MacOS Installation Linux cd ~ curl -sL https://deb.nodesource.com/setup_14.x -o nodesource_setup.sh nano nodesource_setup.sh sudo bash nodesource_setup.sh sudo apt install nodejs node -v Node Version node -v NPM Version NPM is the official Node Package Manager of Node.js, it comes with Node. npm -v","title":"Install Node.js"},{"location":"1.Installation/Manual/4.Nodejs.html#install-project-dependencies","text":"Client // NPM cd Client npm install // Yarn cd Client yarn install Server // NPM cd Server npm install // Yarn cd Server yarn install","title":"Install Project Dependencies"},{"location":"1.Installation/Manual/4.Nodejs.html#run-application","text":"Script // NPM cd Server npm run dev // Yarn cd Server yarn run dev","title":"Run Application"},{"location":"1.Installation/Manual/4.Nodejs.html#running-services","text":"Service Port client 3000 server 4000","title":"Running Services"},{"location":"1.Installation/Manual/4.Nodejs.html#sources","text":"Source Author Link How To Install Node.js on Ubuntu 20.04 Digital Ocean Link","title":"Sources"},{"location":"1.Installation/Manual/5.Nginx.html","text":"Nginx Use Case Nginx was chosen as a web server over Apache because of it's high performance, high concurrency and low resource usage. Nginx Installation Prerequisites Make Two Installation Possible Nginx can be installed via a package manager in Linux or it's source code can be downloaded and build by our self. The advantage of installing Nginx from source is that we can add nginx modules if needed, something that can not be done via a package manager. In the following example, Nginx will be build from source. Installation # Update & upgrade existing packages sudo apt update sudo apt upgrade # Download source from nginx.org wget https://nginx.org/download/nginx-1.20.1.tar.gz tar -zxvf nginx-1.20.1.tar.gz mv nginx-1.20.1 nginx Missing Modules Module Description libpcre3 libpcre3-dev Perl regular expression zlib zlib1g-dev Gzip libssl-dev TLS support libgd-dev WebP support Install Missing Modules # Install Nginx Modules sudo apt install libpcre3 \\ libpcre3-dev \\ zlib1g \\ zlib1g-dev \\ libssl-dev \\ libgd-dev Custom Configuration Flags Information Configurations and modules can be added / modified before the build step. cd nginx ./configure --sbin-path = /usr/bin/nginx \\ --conf-path = /etc/nginx/nginx.conf \\ --error-log-path = /var/log/nginx/error.log \\ --http-log-path = /var/log/nginx/access.log \\ --with-pcre \\ --pid-path = /var/run/nginx.pid \\ --with-http_ssl_module \\ --with-http_image_filter_module \\ --modules-path = /etc/nginx/modules \\ --with-http_v2_module Install Configuration sudo make install Creating Systemctl Script Nginx systemctl script must be in the following location, containing following content. Systemctl Script path /lib/systemd/system/nginx.service Systemctl Script Content [ Unit ] Description = The NGINX HTTP and reverse proxy server After = syslog.target network-online.target remote-fs.target nss-lookup.target Wants = network-online.target [ Service ] Type = forking PIDFile = /run/nginx.pid ExecStartPre = /usr/bin/nginx -t ExecStart = /usr/bin/nginx ExecReload = /usr/bin/nginx -s reload ExecStop = /bin/kill -s QUIT $MAINPID PrivateTmp = true [ Install ] WantedBy = multi-user.target Start Nginx systemctl start nginx Enable Nginx systemctl enable nginx Disable Nginx systemctl disable start Restart Nginx systemctl restart nginx Nginx Status systemctl status nginx Configuration Configuration File Location /etc/nginx/nginx.conf Verify Nginx Configuration nginx -t Nginx default.conf File worker_processes auto ; events { worker_connections 1024 ; } http { include /etc/nginx/conf.d/*.conf ; upstream client { server 127 .0.0.1:3000 ; } upstream server { server 127 .0.0.1:4000 ; } #Virtual server host #Redirect all trafic to HTTPS server { listen 80 ; server_name localhost ; return 301 https:// $host$request_uri ; } server { #Port Nginx should listen to and HTTP version #No SSL is used, actually its TLS listen 443 ssl http2 ; #Define server name, ip address, localhost, domain name, ... server_name localhost ; #Disable SSL ssl_protocols TLSv1 TLSv1.1 TLSv1.2 ; #Optimise cipher suits ssl_prefer_server_ciphers on ; ssl_ciphers ECDH+AESGCM:ECDH+AES256:ECDH+AES128:DH+3DES:!ADH:!AECDH:!MD5 ; #Enable DH(Diffie-Hellman) params, alow to do key exchange with perfect secrety #ssl_dhparam /etc/nginx/ssl/dhparam.pem; #Enable HSTS (HTTP Strict Transport Security) #Header telling to load nothing over HTTP add_header Strict-Transport-Security 'max-age=31536000' always ; #TLS sessions, cache TLS handshake ssl_session_cache shared:SSL:40m ; ssl_session_timeout 4h ; #Specify path to SSL/TLS certificate and public key ssl_certificate /etc/nginx/server.crt ; ssl_certificate_key /etc/nginx/server.key ; #Avoid X-Frame, can leed to clickjacking attack add_header X-Frame-Options \"SAMEORIGIN\" ; #Mitigate XSS attacks add_header X-XSS-Protection \"1; mode=block\" ; # add_header X-Content-Type-Options \"nosniff\" ; location / { proxy_pass http://client ; } location /sockjs-node { proxy_pass http://client ; proxy_http_version 1 .1 ; proxy_set_header Upgrade $http_upgrade ; proxy_set_header Connection \"Upgrade\" ; } location /server { rewrite /api/ ( .* ) / $1 break ; proxy_pass http://server ; } } } Self Signed Certificate Generate TLS Certificate Script sudo vim generate.sh #!bin/bash openssl req \\ -newkey rsa:2048 \\ -x509 \\ -nodes \\ -keyout server.key \\ -new \\ -out server.crt \\ -config ./openssl-custom.cnf \\ -sha256 \\ -days 7300 TLS Information sudo vim openssl-custom.cnf [ req ] default_bits = 2048 prompt = no default_md = sha256 x509_extensions = v3_req distinguished_name = dn [ dn ] C = US ST = KS L = Olathe O = IT OU = IT Department emailAddress = webmaster@example.com CN = localhost [ v3_req ] subjectAltName = @alt_names [ alt_names ] DNS.1 = *.localhost DNS.2 = localhost # copy script to nginx folder cp generate.sh /etc/nginx # TLS information file cp openssl-custom.cnf /etc/nginx # Execute script, it generates TLS certificate sudo bash ./generate.sh Sources Source Author Link Installing Nginx Nginx Documentation Link Systemd freedesktop.org Link Nginx Init Script Nginx Documentation Link Nginx Systemctl Script Nginx Documentation Link","title":"Nginx"},{"location":"1.Installation/Manual/5.Nginx.html#nginx","text":"","title":"Nginx"},{"location":"1.Installation/Manual/5.Nginx.html#use-case","text":"Nginx was chosen as a web server over Apache because of it's high performance, high concurrency and low resource usage.","title":"Use Case"},{"location":"1.Installation/Manual/5.Nginx.html#nginx-installation-prerequisites","text":"Make","title":"Nginx Installation Prerequisites"},{"location":"1.Installation/Manual/5.Nginx.html#two-installation-possible","text":"Nginx can be installed via a package manager in Linux or it's source code can be downloaded and build by our self. The advantage of installing Nginx from source is that we can add nginx modules if needed, something that can not be done via a package manager. In the following example, Nginx will be build from source.","title":"Two Installation Possible"},{"location":"1.Installation/Manual/5.Nginx.html#installation","text":"# Update & upgrade existing packages sudo apt update sudo apt upgrade # Download source from nginx.org wget https://nginx.org/download/nginx-1.20.1.tar.gz tar -zxvf nginx-1.20.1.tar.gz mv nginx-1.20.1 nginx","title":"Installation"},{"location":"1.Installation/Manual/5.Nginx.html#missing-modules","text":"Module Description libpcre3 libpcre3-dev Perl regular expression zlib zlib1g-dev Gzip libssl-dev TLS support libgd-dev WebP support Install Missing Modules # Install Nginx Modules sudo apt install libpcre3 \\ libpcre3-dev \\ zlib1g \\ zlib1g-dev \\ libssl-dev \\ libgd-dev","title":"Missing Modules"},{"location":"1.Installation/Manual/5.Nginx.html#custom-configuration-flags","text":"Information Configurations and modules can be added / modified before the build step. cd nginx ./configure --sbin-path = /usr/bin/nginx \\ --conf-path = /etc/nginx/nginx.conf \\ --error-log-path = /var/log/nginx/error.log \\ --http-log-path = /var/log/nginx/access.log \\ --with-pcre \\ --pid-path = /var/run/nginx.pid \\ --with-http_ssl_module \\ --with-http_image_filter_module \\ --modules-path = /etc/nginx/modules \\ --with-http_v2_module Install Configuration sudo make install","title":"Custom Configuration Flags"},{"location":"1.Installation/Manual/5.Nginx.html#creating-systemctl-script","text":"Nginx systemctl script must be in the following location, containing following content. Systemctl Script path /lib/systemd/system/nginx.service Systemctl Script Content [ Unit ] Description = The NGINX HTTP and reverse proxy server After = syslog.target network-online.target remote-fs.target nss-lookup.target Wants = network-online.target [ Service ] Type = forking PIDFile = /run/nginx.pid ExecStartPre = /usr/bin/nginx -t ExecStart = /usr/bin/nginx ExecReload = /usr/bin/nginx -s reload ExecStop = /bin/kill -s QUIT $MAINPID PrivateTmp = true [ Install ] WantedBy = multi-user.target Start Nginx systemctl start nginx Enable Nginx systemctl enable nginx Disable Nginx systemctl disable start Restart Nginx systemctl restart nginx Nginx Status systemctl status nginx","title":"Creating Systemctl Script"},{"location":"1.Installation/Manual/5.Nginx.html#configuration","text":"Configuration File Location /etc/nginx/nginx.conf Verify Nginx Configuration nginx -t Nginx default.conf File worker_processes auto ; events { worker_connections 1024 ; } http { include /etc/nginx/conf.d/*.conf ; upstream client { server 127 .0.0.1:3000 ; } upstream server { server 127 .0.0.1:4000 ; } #Virtual server host #Redirect all trafic to HTTPS server { listen 80 ; server_name localhost ; return 301 https:// $host$request_uri ; } server { #Port Nginx should listen to and HTTP version #No SSL is used, actually its TLS listen 443 ssl http2 ; #Define server name, ip address, localhost, domain name, ... server_name localhost ; #Disable SSL ssl_protocols TLSv1 TLSv1.1 TLSv1.2 ; #Optimise cipher suits ssl_prefer_server_ciphers on ; ssl_ciphers ECDH+AESGCM:ECDH+AES256:ECDH+AES128:DH+3DES:!ADH:!AECDH:!MD5 ; #Enable DH(Diffie-Hellman) params, alow to do key exchange with perfect secrety #ssl_dhparam /etc/nginx/ssl/dhparam.pem; #Enable HSTS (HTTP Strict Transport Security) #Header telling to load nothing over HTTP add_header Strict-Transport-Security 'max-age=31536000' always ; #TLS sessions, cache TLS handshake ssl_session_cache shared:SSL:40m ; ssl_session_timeout 4h ; #Specify path to SSL/TLS certificate and public key ssl_certificate /etc/nginx/server.crt ; ssl_certificate_key /etc/nginx/server.key ; #Avoid X-Frame, can leed to clickjacking attack add_header X-Frame-Options \"SAMEORIGIN\" ; #Mitigate XSS attacks add_header X-XSS-Protection \"1; mode=block\" ; # add_header X-Content-Type-Options \"nosniff\" ; location / { proxy_pass http://client ; } location /sockjs-node { proxy_pass http://client ; proxy_http_version 1 .1 ; proxy_set_header Upgrade $http_upgrade ; proxy_set_header Connection \"Upgrade\" ; } location /server { rewrite /api/ ( .* ) / $1 break ; proxy_pass http://server ; } } }","title":"Configuration"},{"location":"1.Installation/Manual/5.Nginx.html#self-signed-certificate","text":"Generate TLS Certificate Script sudo vim generate.sh #!bin/bash openssl req \\ -newkey rsa:2048 \\ -x509 \\ -nodes \\ -keyout server.key \\ -new \\ -out server.crt \\ -config ./openssl-custom.cnf \\ -sha256 \\ -days 7300 TLS Information sudo vim openssl-custom.cnf [ req ] default_bits = 2048 prompt = no default_md = sha256 x509_extensions = v3_req distinguished_name = dn [ dn ] C = US ST = KS L = Olathe O = IT OU = IT Department emailAddress = webmaster@example.com CN = localhost [ v3_req ] subjectAltName = @alt_names [ alt_names ] DNS.1 = *.localhost DNS.2 = localhost # copy script to nginx folder cp generate.sh /etc/nginx # TLS information file cp openssl-custom.cnf /etc/nginx # Execute script, it generates TLS certificate sudo bash ./generate.sh","title":"Self Signed Certificate"},{"location":"1.Installation/Manual/5.Nginx.html#sources","text":"Source Author Link Installing Nginx Nginx Documentation Link Systemd freedesktop.org Link Nginx Init Script Nginx Documentation Link Nginx Systemctl Script Nginx Documentation Link","title":"Sources"},{"location":"2.Architecture/1.client.html","text":"Client Comming Soon.","title":"Client"},{"location":"2.Architecture/1.client.html#client","text":"","title":"Client"},{"location":"2.Architecture/1.client.html#comming-soon","text":"","title":"Comming Soon."},{"location":"2.Architecture/2.backend.html","text":"Backend Model, View, Controller The most basic architecture for the application, would be a monolithic MVC architecture. In the front of the application would sit a reverse proxy, like Nginx, Apache or Envoy, it redirects HTTP and HTTPS trafic to the frontend or the backend. As for the databases, there would be a MongoDB database and Redis in-memory database. MongoDB is the principle database and Redis is used for rate-limiting and for storing TTL information. Personal Diagram Scaling Architecture This second architecture is almost as the first one, except that it was design with redundancy in mind. The MongoDB databases would form a cluster, with quorum master worker election. In this design, instead of using a reverse proxy, the traffic would be handled via a load balancer, it would balance the incoming traffic to the services. The best way of distributing traffic would be with the Round Robin algorithm. Least Connect, Weight and IP Hash Algorithm are also possible but very specific. Personal Diagram Microservices The microservices architecture is mostly used by large teams in big companies, it prones service ownership, fast delivery and large scale. The advantage of microservices is that each service does just on thing very well. Each service has it's own database and databases communicates via the Saga Pattern between them. Service communication is handled via a service bus, like Apache Kafka or RabbitMQ. Authentication and authorization would be handled via an access management service like Keycloak . The API Gateway, the only service which calls the microservices, would allow to decrease backend complexity to the frontend. Personal Diagram CI/CD CI/CD URI Description Ansible Link Jenkins Link Provisioning Provisioner URI Description Terraform Link Monitoring Monitoring URI Description Grafana Link Prometheus Link Jaeger Link Service Bus Service Bus URI Description Kafka Link RabbitMQ Link API API URI Description REST Link GraphQL Link gRPC Link Containers Container URI Description Docker Link Kubernetes Link Istio Link Databases Database URI Description MySQL Link PostgreSQL Link MongoDB Link Redis Link Neo4j Link Elasticsearch Link Solr Link Access Management Service URI Description Keycloak Link Chaos Engineering Tool URI Description Chaos Toolkit Link","title":"Backend"},{"location":"2.Architecture/2.backend.html#backend","text":"","title":"Backend"},{"location":"2.Architecture/2.backend.html#model-view-controller","text":"The most basic architecture for the application, would be a monolithic MVC architecture. In the front of the application would sit a reverse proxy, like Nginx, Apache or Envoy, it redirects HTTP and HTTPS trafic to the frontend or the backend. As for the databases, there would be a MongoDB database and Redis in-memory database. MongoDB is the principle database and Redis is used for rate-limiting and for storing TTL information. Personal Diagram","title":"Model, View, Controller"},{"location":"2.Architecture/2.backend.html#scaling-architecture","text":"This second architecture is almost as the first one, except that it was design with redundancy in mind. The MongoDB databases would form a cluster, with quorum master worker election. In this design, instead of using a reverse proxy, the traffic would be handled via a load balancer, it would balance the incoming traffic to the services. The best way of distributing traffic would be with the Round Robin algorithm. Least Connect, Weight and IP Hash Algorithm are also possible but very specific. Personal Diagram","title":"Scaling Architecture"},{"location":"2.Architecture/2.backend.html#microservices","text":"The microservices architecture is mostly used by large teams in big companies, it prones service ownership, fast delivery and large scale. The advantage of microservices is that each service does just on thing very well. Each service has it's own database and databases communicates via the Saga Pattern between them. Service communication is handled via a service bus, like Apache Kafka or RabbitMQ. Authentication and authorization would be handled via an access management service like Keycloak . The API Gateway, the only service which calls the microservices, would allow to decrease backend complexity to the frontend. Personal Diagram","title":"Microservices"},{"location":"2.Architecture/2.backend.html#cicd","text":"CI/CD URI Description Ansible Link Jenkins Link","title":"CI/CD"},{"location":"2.Architecture/2.backend.html#provisioning","text":"Provisioner URI Description Terraform Link","title":"Provisioning"},{"location":"2.Architecture/2.backend.html#monitoring","text":"Monitoring URI Description Grafana Link Prometheus Link Jaeger Link","title":"Monitoring"},{"location":"2.Architecture/2.backend.html#service-bus","text":"Service Bus URI Description Kafka Link RabbitMQ Link","title":"Service Bus"},{"location":"2.Architecture/2.backend.html#api","text":"API URI Description REST Link GraphQL Link gRPC Link","title":"API"},{"location":"2.Architecture/2.backend.html#containers","text":"Container URI Description Docker Link Kubernetes Link Istio Link","title":"Containers"},{"location":"2.Architecture/2.backend.html#databases","text":"Database URI Description MySQL Link PostgreSQL Link MongoDB Link Redis Link Neo4j Link Elasticsearch Link Solr Link","title":"Databases"},{"location":"2.Architecture/2.backend.html#access-management","text":"Service URI Description Keycloak Link","title":"Access Management"},{"location":"2.Architecture/2.backend.html#chaos-engineering","text":"Tool URI Description Chaos Toolkit Link","title":"Chaos Engineering"},{"location":"3.Client/1.next.html","text":"Next Client Side Rendering Client side rendering is faster at rendering after initial load, it comes with greateuser experience, good variety of libraries and frameworks. Client side rendering doesn't shine with initial load time and SEO. For the moment, search engines like Bing, Yandex and more struggle to index web pages with a lot of Javascript, their ranking performs badly. Personal Diagram Server Side Rendering In contrast to client side rendering, Javascript is rendered as HTML, making it possible to be indexed in the first round trip by search engines and improve SEO. Initial load times are improved, allowing users to get content much faster in opposite with client side rendering. Decreasing load time, improves SEO and converts more clients. The process of server side rendering after initial load is higher than with client side rendering. Personal Diagram Choice For this project, Next.js was chosen over Gatsby and create-react-app for it's server-side rendering capabilities and for it's SEO support. Personal Diagram Libraries Library URI Description Typescript Link Typescript is a superset of Javascript, it allows to add types in development and detect compilation errors at compile time. Next.js Link Next.js is a server side rendering framework built on top of React, it optimizes search engine results. Sources Source Author URI Apollo Server and Client Auth Example Next.js Example Link Apollo Client State Management Example Apollo Example Link Strongly Typed Next.js Michael Stromer Link Demo Ecommerce Vercel Link","title":"Next"},{"location":"3.Client/1.next.html#next","text":"","title":"Next"},{"location":"3.Client/1.next.html#client-side-rendering","text":"Client side rendering is faster at rendering after initial load, it comes with greateuser experience, good variety of libraries and frameworks. Client side rendering doesn't shine with initial load time and SEO. For the moment, search engines like Bing, Yandex and more struggle to index web pages with a lot of Javascript, their ranking performs badly. Personal Diagram","title":"Client Side Rendering"},{"location":"3.Client/1.next.html#server-side-rendering","text":"In contrast to client side rendering, Javascript is rendered as HTML, making it possible to be indexed in the first round trip by search engines and improve SEO. Initial load times are improved, allowing users to get content much faster in opposite with client side rendering. Decreasing load time, improves SEO and converts more clients. The process of server side rendering after initial load is higher than with client side rendering. Personal Diagram","title":"Server Side Rendering"},{"location":"3.Client/1.next.html#choice","text":"For this project, Next.js was chosen over Gatsby and create-react-app for it's server-side rendering capabilities and for it's SEO support. Personal Diagram","title":"Choice"},{"location":"3.Client/1.next.html#libraries","text":"Library URI Description Typescript Link Typescript is a superset of Javascript, it allows to add types in development and detect compilation errors at compile time. Next.js Link Next.js is a server side rendering framework built on top of React, it optimizes search engine results.","title":"Libraries"},{"location":"3.Client/1.next.html#sources","text":"Source Author URI Apollo Server and Client Auth Example Next.js Example Link Apollo Client State Management Example Apollo Example Link Strongly Typed Next.js Michael Stromer Link Demo Ecommerce Vercel Link","title":"Sources"},{"location":"3.Client/2.ui.html","text":"UI Library Material-UI Material-UI's built-in component allows to create pages way faster then building components from the ground up, like with styled components . Other UI Solution The application could have been build with Chakra and Tailwindcss , but there was a learning curve to it, furthermost, most end users already familiar and appreciate Google's material design, that's why the project is build with Material-UI. Libraries Library URI Description Material-UI Link Material-UI is a UI library component kit built on Google's material design system. React Hook Form Link React Hook Form is a light and performant library built on top of React hooks, it makes form validation very easy.","title":"UI Library"},{"location":"3.Client/2.ui.html#ui-library","text":"","title":"UI Library"},{"location":"3.Client/2.ui.html#material-ui","text":"Material-UI's built-in component allows to create pages way faster then building components from the ground up, like with styled components .","title":"Material-UI"},{"location":"3.Client/2.ui.html#other-ui-solution","text":"The application could have been build with Chakra and Tailwindcss , but there was a learning curve to it, furthermost, most end users already familiar and appreciate Google's material design, that's why the project is build with Material-UI.","title":"Other UI Solution"},{"location":"3.Client/2.ui.html#libraries","text":"Library URI Description Material-UI Link Material-UI is a UI library component kit built on Google's material design system. React Hook Form Link React Hook Form is a light and performant library built on top of React hooks, it makes form validation very easy.","title":"Libraries"},{"location":"3.Client/3.state.html","text":"State Management Apollo Client Instead of writing imperative Javascript, specially for events, state management allows in a declarative way to respond to certain events. State management allows to manage the growing complexity of current modern web applications. Apollo client is simply a client library for GraphQL backends, available for React, Vue and Angular, it comes with a cache and there are almost no boilerplate involving it. Image: Inspired From Apollo Docs Basic Usage(recommended) Advanced Initializing state Cache Policies N/A Local state management Reactive variables + Cache Policies N/A State update cache.readQuery + cache.writeQuery cache.modify, cache.evict Pagination cache.radQuery + cache.writeQuery Cache Policies Apollo 3 Presentation, Khalil Stemmer Other State Management Solution The application could have used a different state management library, like Redux. Actually it did at the beginning, but the code for just writing some actions and some reducers introduced huge boilerplate to maintain, it represents in most cases for ~40-60% of an app frontend's code. Due to the large boilerplate code, Apollo Client was choosed over Redux Libraries Library URI Description Apollo Client Link Apollo Client is a GraphQL client library, it's simple API allows to use it as a local state management tool and to quickly write business code which mathers, in comparison of Redux's long boilerplate. GraphQL Code Generator Link GraphQL Code Generator allows to convert a GraphQL API into Typescript.","title":"State Management"},{"location":"3.Client/3.state.html#state-management","text":"","title":"State Management"},{"location":"3.Client/3.state.html#apollo-client","text":"Instead of writing imperative Javascript, specially for events, state management allows in a declarative way to respond to certain events. State management allows to manage the growing complexity of current modern web applications. Apollo client is simply a client library for GraphQL backends, available for React, Vue and Angular, it comes with a cache and there are almost no boilerplate involving it. Image: Inspired From Apollo Docs Basic Usage(recommended) Advanced Initializing state Cache Policies N/A Local state management Reactive variables + Cache Policies N/A State update cache.readQuery + cache.writeQuery cache.modify, cache.evict Pagination cache.radQuery + cache.writeQuery Cache Policies Apollo 3 Presentation, Khalil Stemmer","title":"Apollo Client"},{"location":"3.Client/3.state.html#other-state-management-solution","text":"The application could have used a different state management library, like Redux. Actually it did at the beginning, but the code for just writing some actions and some reducers introduced huge boilerplate to maintain, it represents in most cases for ~40-60% of an app frontend's code. Due to the large boilerplate code, Apollo Client was choosed over Redux","title":"Other State Management Solution"},{"location":"3.Client/3.state.html#libraries","text":"Library URI Description Apollo Client Link Apollo Client is a GraphQL client library, it's simple API allows to use it as a local state management tool and to quickly write business code which mathers, in comparison of Redux's long boilerplate. GraphQL Code Generator Link GraphQL Code Generator allows to convert a GraphQL API into Typescript.","title":"Libraries"},{"location":"3.Client/4.Guard.html","text":"Route Guard Protected Pages On the client, unauthorized pages are protected with the help of an Higher Order Component, also Called HOC, it filters the requests based on specific state, like if the user is an admin, or if he is authenticated, and so on. The following code comes from Ben Awad on Github. HOC import React from \"react\" ; // Guard import redirect from \"./redirect\" ; import { NextContextWithApollo } from \"./nextContextWithApollo\" ; // Apollo import { GetCurrentUserDocument , GetCurrentUserQuery } from \"../Graphql/index\" ; // Apollo State import { user } from \"../Apollo/state/user/index\" ; // ================================================================================ export const withAuth = < T extends object > ( C : React.FC < T > ) => class AuthComponent extends React . Component < T > { static async getInitialProps ({ apolloClient , ... ctx } : NextContextWithApollo ) : Promise < { user : {} | null } > { const token = ctx ? . req ? . headers ? . cookie ? . split ( \"token=\" )[ 1 ] ? . split ( \";\" )[ 0 ]; const result = await apolloClient ? . query < GetCurrentUserQuery > ({ query : GetCurrentUserDocument , context : { credentials : \"include\" , headers : { token : token , }, }, }); user ({ _id : result?.data?.getCurrentUser?._id , username : result?.data?.getCurrentUser?.username , role : result?.data?.getCurrentUser?.role , profileImageUrl : result?.data?.getCurrentUser?.profileImageUrl , }); const isAdmin = result . data ? . getCurrentUser ? . role === \"admin\" ; if ( ! result || ! result . data || ! result . data ? . getCurrentUser || ! isAdmin ) { redirect ( ctx , \"/register\" ); return { user : null , }; } return { user : result?.data?.getCurrentUser , }; } render () { return < C {... this . props } /> ; } }; Redirect import Router from \"next/router\" ; export default ( context : any , target : string ) => { if ( context . res ) { context . res . writeHead ( 302 , { Location : target }); context . res . end (); } else { Router . replace ( target ); } }; Types import { NextPageContext } from \"next\" ; import { ApolloClient , NormalizedCacheObject } from \"@apollo/client\" ; export interface NextContextWithApollo extends NextPageContext { apolloClient : ApolloClient < NormalizedCacheObject > ; githubApolloClient : ApolloClient < NormalizedCacheObject > ; } Example // Auth import { withAuth } from \"../../../Guard/withAuth\" ; export default withApollo ( withAuth ( index ), { getDataFromTree }); Sources Source Author URI Next.js Page Guard Ben Awad Link","title":"Route Guard"},{"location":"3.Client/4.Guard.html#route-guard","text":"","title":"Route Guard"},{"location":"3.Client/4.Guard.html#protected-pages","text":"On the client, unauthorized pages are protected with the help of an Higher Order Component, also Called HOC, it filters the requests based on specific state, like if the user is an admin, or if he is authenticated, and so on. The following code comes from Ben Awad on Github. HOC import React from \"react\" ; // Guard import redirect from \"./redirect\" ; import { NextContextWithApollo } from \"./nextContextWithApollo\" ; // Apollo import { GetCurrentUserDocument , GetCurrentUserQuery } from \"../Graphql/index\" ; // Apollo State import { user } from \"../Apollo/state/user/index\" ; // ================================================================================ export const withAuth = < T extends object > ( C : React.FC < T > ) => class AuthComponent extends React . Component < T > { static async getInitialProps ({ apolloClient , ... ctx } : NextContextWithApollo ) : Promise < { user : {} | null } > { const token = ctx ? . req ? . headers ? . cookie ? . split ( \"token=\" )[ 1 ] ? . split ( \";\" )[ 0 ]; const result = await apolloClient ? . query < GetCurrentUserQuery > ({ query : GetCurrentUserDocument , context : { credentials : \"include\" , headers : { token : token , }, }, }); user ({ _id : result?.data?.getCurrentUser?._id , username : result?.data?.getCurrentUser?.username , role : result?.data?.getCurrentUser?.role , profileImageUrl : result?.data?.getCurrentUser?.profileImageUrl , }); const isAdmin = result . data ? . getCurrentUser ? . role === \"admin\" ; if ( ! result || ! result . data || ! result . data ? . getCurrentUser || ! isAdmin ) { redirect ( ctx , \"/register\" ); return { user : null , }; } return { user : result?.data?.getCurrentUser , }; } render () { return < C {... this . props } /> ; } }; Redirect import Router from \"next/router\" ; export default ( context : any , target : string ) => { if ( context . res ) { context . res . writeHead ( 302 , { Location : target }); context . res . end (); } else { Router . replace ( target ); } }; Types import { NextPageContext } from \"next\" ; import { ApolloClient , NormalizedCacheObject } from \"@apollo/client\" ; export interface NextContextWithApollo extends NextPageContext { apolloClient : ApolloClient < NormalizedCacheObject > ; githubApolloClient : ApolloClient < NormalizedCacheObject > ; } Example // Auth import { withAuth } from \"../../../Guard/withAuth\" ; export default withApollo ( withAuth ( index ), { getDataFromTree });","title":"Protected Pages"},{"location":"3.Client/4.Guard.html#sources","text":"Source Author URI Next.js Page Guard Ben Awad Link","title":"Sources"},{"location":"3.Client/5.upload.html","text":"File Upload Apollo-Upload-Client The Apollo-Upload-Package allows to integrate file upload into GraphqQL. Initialization // Apollo import { InMemoryCache , ApolloClient } from \"@apollo/client\" ; // Apollo State import { ui } from \"./state/ui\" ; import { user } from \"./state/user/index\" ; // Upload import { createUploadLink } from \"apollo-upload-client\" ; // SSR import { withApollo } from \"next-apollo\" ; // ======================================================================================================== const cache = new InMemoryCache ({ typePolicies : { Query : { fields : { ui : { read () { return ui (); }, }, user : { read () { return user (); }, }, }, }, }, }); let token ; if ( typeof window !== \"undefined\" ) { token = window . localStorage . getItem ( \"token\" ); } const link = createUploadLink ({ uri : \"http://localhost:4000/graphql\" , credentials : \"include\" , headers : { token : token , }, }); export const apolloClient = new ApolloClient ({ link : link , headers : { token : token , }, cache , credentials : \"include\" , ssrMode : true , }); export default withApollo ( apolloClient ); Libraries Library URI Description Apollo-Upload-Client Link Apollo-Upload-Client is a wrapper over the uri property of ApolloClient. By implementing it, file upload can be achieved","title":"File Upload"},{"location":"3.Client/5.upload.html#file-upload","text":"","title":"File Upload"},{"location":"3.Client/5.upload.html#apollo-upload-client","text":"The Apollo-Upload-Package allows to integrate file upload into GraphqQL. Initialization // Apollo import { InMemoryCache , ApolloClient } from \"@apollo/client\" ; // Apollo State import { ui } from \"./state/ui\" ; import { user } from \"./state/user/index\" ; // Upload import { createUploadLink } from \"apollo-upload-client\" ; // SSR import { withApollo } from \"next-apollo\" ; // ======================================================================================================== const cache = new InMemoryCache ({ typePolicies : { Query : { fields : { ui : { read () { return ui (); }, }, user : { read () { return user (); }, }, }, }, }, }); let token ; if ( typeof window !== \"undefined\" ) { token = window . localStorage . getItem ( \"token\" ); } const link = createUploadLink ({ uri : \"http://localhost:4000/graphql\" , credentials : \"include\" , headers : { token : token , }, }); export const apolloClient = new ApolloClient ({ link : link , headers : { token : token , }, cache , credentials : \"include\" , ssrMode : true , }); export default withApollo ( apolloClient );","title":"Apollo-Upload-Client"},{"location":"3.Client/5.upload.html#libraries","text":"Library URI Description Apollo-Upload-Client Link Apollo-Upload-Client is a wrapper over the uri property of ApolloClient. By implementing it, file upload can be achieved","title":"Libraries"},{"location":"3.Client/6.testing.html","text":"Testing","title":"Testing"},{"location":"3.Client/6.testing.html#testing","text":"","title":"Testing"},{"location":"3.Client/7.performance.html","text":"Performance Debouncing Logic What makes React great, is that it implements the concepts of the shadow DOM and the DOM well together. State in React is updated in the shadow Dom before updating the real DOM, in other words, rendering the DOM is costly, Input events and scroll events for example have tendencies to be fired after each change, to save performance, they should be minimized. By using the setTimeout function in the useEffect hook, events are updated only after 50 milliseconds. Between events, if they are still firing, the timer will reset and wait for another 50 milliseconds. The wait time is not cumulative. React useEffect useEffect (() => { const timer = setTimeout (() => { logic }, 50 ); return () => { clearTimeout ( timer ); }; }, [ state ]); Code Splitting By default, Next.js comes with code splitting. This means that the Javascript code on the login page will only be fetched, when and only the user navigates to the login page. Image Lazy Loading The same idea as code splitting refers to image lazy loading. Instead of fetching all the assets before the user needs them, Nextjs fetches with the help of the Image module, only images that are needed on the page. Note that the image lazy loading process can also be handled in plain Javascript with the help of the Intersection Observer API. An example can be found on one of my other projects 1 . Next.js import Image from \"next/image\" < Image width = { 300 } height = { 250 } className = { \"css class\" } onClick = {() => \"event\" } src = { \"imageUrl\" } title = { \"title\" } /> Vanilla Javascript const targets = document . querySelectorAll ( \"[data-lazy]\" ); const lazyLoad = ( target ) => { const io = new IntersectionObserver (( entries , observer ) => { entries . forEach (( entry ) => { if ( entry . isIntersecting ) { const img = entry . target ; const srcset = img . getAttribute ( \"data-lazy\" ); img . setAttribute ( \"srcset\" , srcset ); observer . disconnect (); } }); }); io . observe ( target ); }; targets . forEach ( lazyLoad ); WebP Support WebP is an efficient image coding format, it performs on average 30% better than JPEG or PNG for images size. The choice was clear, use only WebP. WebP as a 89% market share, but on some web browsers it's still new, like for example Firefox on Android, it supports WebP only since January 26 2021. There is a way to show JPEG/PNG images to users who haven't yet updated their web browsers and WebP images to those who have updated their software, it will involve the Intersection Observer API, but this is a solution for another time. Component Memoization Comming Soon. Function Memoization Comming Soon. Data Memoization Comming Soon. Server Side Rendering serverSideProps and getStaticProps https://github.com/besSejrani/PWA-Camera/blob/master/src/js/utils/lazyImages.ts \u21a9","title":"Performance"},{"location":"3.Client/7.performance.html#performance","text":"","title":"Performance"},{"location":"3.Client/7.performance.html#debouncing-logic","text":"What makes React great, is that it implements the concepts of the shadow DOM and the DOM well together. State in React is updated in the shadow Dom before updating the real DOM, in other words, rendering the DOM is costly, Input events and scroll events for example have tendencies to be fired after each change, to save performance, they should be minimized. By using the setTimeout function in the useEffect hook, events are updated only after 50 milliseconds. Between events, if they are still firing, the timer will reset and wait for another 50 milliseconds. The wait time is not cumulative. React useEffect useEffect (() => { const timer = setTimeout (() => { logic }, 50 ); return () => { clearTimeout ( timer ); }; }, [ state ]);","title":"Debouncing Logic"},{"location":"3.Client/7.performance.html#code-splitting","text":"By default, Next.js comes with code splitting. This means that the Javascript code on the login page will only be fetched, when and only the user navigates to the login page.","title":"Code Splitting"},{"location":"3.Client/7.performance.html#image-lazy-loading","text":"The same idea as code splitting refers to image lazy loading. Instead of fetching all the assets before the user needs them, Nextjs fetches with the help of the Image module, only images that are needed on the page. Note that the image lazy loading process can also be handled in plain Javascript with the help of the Intersection Observer API. An example can be found on one of my other projects 1 . Next.js import Image from \"next/image\" < Image width = { 300 } height = { 250 } className = { \"css class\" } onClick = {() => \"event\" } src = { \"imageUrl\" } title = { \"title\" } /> Vanilla Javascript const targets = document . querySelectorAll ( \"[data-lazy]\" ); const lazyLoad = ( target ) => { const io = new IntersectionObserver (( entries , observer ) => { entries . forEach (( entry ) => { if ( entry . isIntersecting ) { const img = entry . target ; const srcset = img . getAttribute ( \"data-lazy\" ); img . setAttribute ( \"srcset\" , srcset ); observer . disconnect (); } }); }); io . observe ( target ); }; targets . forEach ( lazyLoad );","title":"Image Lazy Loading"},{"location":"3.Client/7.performance.html#webp-support","text":"WebP is an efficient image coding format, it performs on average 30% better than JPEG or PNG for images size. The choice was clear, use only WebP. WebP as a 89% market share, but on some web browsers it's still new, like for example Firefox on Android, it supports WebP only since January 26 2021. There is a way to show JPEG/PNG images to users who haven't yet updated their web browsers and WebP images to those who have updated their software, it will involve the Intersection Observer API, but this is a solution for another time.","title":"WebP Support"},{"location":"3.Client/7.performance.html#component-memoization","text":"Comming Soon.","title":"Component Memoization"},{"location":"3.Client/7.performance.html#function-memoization","text":"Comming Soon.","title":"Function Memoization"},{"location":"3.Client/7.performance.html#data-memoization","text":"Comming Soon.","title":"Data Memoization"},{"location":"3.Client/7.performance.html#server-side-rendering","text":"serverSideProps and getStaticProps https://github.com/besSejrani/PWA-Camera/blob/master/src/js/utils/lazyImages.ts \u21a9","title":"Server Side Rendering"},{"location":"3.Client/8.seo.html","text":"SEO Performance Comming Soon. Semantics Comming Soon. Open Graph Protocols Comming Soon.","title":"SEO"},{"location":"3.Client/8.seo.html#seo","text":"","title":"SEO"},{"location":"3.Client/8.seo.html#performance","text":"Comming Soon.","title":"Performance"},{"location":"3.Client/8.seo.html#semantics","text":"Comming Soon.","title":"Semantics"},{"location":"3.Client/8.seo.html#open-graph-protocols","text":"Comming Soon.","title":"Open Graph Protocols"},{"location":"3.Client/9.configuration.html","text":"Configuration Relative Imports Importing long relative path for a lot of components can be a long taking and tedious task, that's why in the tsconfig.jsonpaths to relative folder have been already set. Tsconfig.json { \"compilerOptions\" : { \"baseUrl\" : \".\" , \"@Components/*\" : [ \"Components/*\" ], \"@Apollo/*\" : [ \"Apollo/*\" ], \"@Graphql/*\" : [ \"Graphql/*\" ], \"@Hook/*\" : [ \"Hook/*\" ], \"@Guard/*\" : [ \"Guard/*\" ] } } Example //Apollo import { useGetProductsQuery } from \"@Graphql/index\" ; // SSR import withApollo from \"@Apollo/ssr\" ; Environement Variables By creating an .env environment file, it's possible to add information and then retrieve it in the application at any time. It behaves like a central store, it's perfect for storing repeatable information like URLs. Warning All information exposed in Next.js are not private, every one can see them. Do not store confidential information on Next via environement variables. Environment variables must start with a prefix of NEXT_ .env # Development URL that Apollo Client calls to contact the server for accessing GraphQL NEXT_PUBLIC_DEVELOPMENT_SERVER = http://localhost:4000/graphql # Production URL that Apollo Client calls to contact the server for accessing GraphQL NEXT_PUBLIC_PRODUCTION_SERVER = https://blueberryshop.herokuapp.com/graphql # Stripe public key, it doesn't matter if people can see it. NEXT_PUBLIC_STRIPE_TEST_KEY = pk_test_wY7bgBZbTJSklGmANSZbI4bf00cG39wwlu GraphQL Code Generator Since all the backend is written in Typescript, it would be great to use those same types in the the frontend. That's exactly what GraphQL Code Generator does, it converts the backend API into Typescript. GraphQL Code Generator needs raw .graphql files to interact with the server. .graphql mutation AddProfilePicture($picture : Upload!) { addProfilePicture(picture : $picture) } GraphQL Code Generator needs a configuration file, named codegen.yaml, it explains to it, how to reach the server and how to retrieve the types. For the project, there are two files, one for development and one for production. GraphQL Code Generator Development overwrite : true schema : ${NEXT_PUBLIC_DEVELOPMENT_SERVER} documents : \"./Graphql/**/**/*.graphql\" # watch: true generates : Graphql/graphql-hooks.ts : plugins : - \"typescript\" - \"typescript-operations\" - \"typescript-react-apollo\" config : withHOC : false withComponent : false withHooks : true GraphQL Code Generator Production overwrite : true schema : ${NEXT_PUBLIC_PRODUCTION_SERVER} documents : \"./Graphql/**/**/*.graphql\" # watch: true generates : Graphql/graphql-hooks.ts : plugins : - \"typescript\" - \"typescript-operations\" - \"typescript-react-apollo\" config : withHOC : false withComponent : false withHooks : true In the package.json file on the client, there is a generate script to execute. The server must be running, otherwise the action will fail. script \"generate:dev\" : \"graphql-codegen -r dotenv/config --config ./codegen.yaml\" , \"generate:prod\" : \"graphql-codegen -r dotenv/config --config ./codegenProd.yaml\" , Eslint & Prettier Having structured and organized code isn't a nice have, but it's a must. With the help of Eslint, a javascript linter and Prettier, a file formater, it is possible to achieve structured code organization, from solo developer to large teams. Install development dependencies yarn add -D eslint prettier eslint-plugin-prettier eslint-config-prettier eslint-plugin-node eslint-config-node Install Airbnb Style Guide npx install-peerdeps --dev eslint-config-airbnb The Airbnb style guide is a popular set of rules defined by Airbnb for Javascript Create Eslint Configuration Fike eslint --init Eslint Configuration env : browser : true es2021 : true extends : - \"eslint:recommended\" - \"plugin:react/recommended\" - \"plugin:@typescript-eslint/recommended\" - \"airbnb\" parser : \"@typescript-eslint/parser\" parserOptions : ecmaFeatures : jsx : true ecmaVersion : 12 sourceType : module plugins : - react - \"@typescript-eslint\" - \"prettier\" rules : { \"quotes\" : [ \"error\" , \"double\" ], \"no-unused-vars\" : \"warn\" , \"no-console\" : \"off\" , }","title":"Configuration"},{"location":"3.Client/9.configuration.html#configuration","text":"","title":"Configuration"},{"location":"3.Client/9.configuration.html#relative-imports","text":"Importing long relative path for a lot of components can be a long taking and tedious task, that's why in the tsconfig.jsonpaths to relative folder have been already set. Tsconfig.json { \"compilerOptions\" : { \"baseUrl\" : \".\" , \"@Components/*\" : [ \"Components/*\" ], \"@Apollo/*\" : [ \"Apollo/*\" ], \"@Graphql/*\" : [ \"Graphql/*\" ], \"@Hook/*\" : [ \"Hook/*\" ], \"@Guard/*\" : [ \"Guard/*\" ] } } Example //Apollo import { useGetProductsQuery } from \"@Graphql/index\" ; // SSR import withApollo from \"@Apollo/ssr\" ;","title":"Relative Imports"},{"location":"3.Client/9.configuration.html#environement-variables","text":"By creating an .env environment file, it's possible to add information and then retrieve it in the application at any time. It behaves like a central store, it's perfect for storing repeatable information like URLs. Warning All information exposed in Next.js are not private, every one can see them. Do not store confidential information on Next via environement variables. Environment variables must start with a prefix of NEXT_ .env # Development URL that Apollo Client calls to contact the server for accessing GraphQL NEXT_PUBLIC_DEVELOPMENT_SERVER = http://localhost:4000/graphql # Production URL that Apollo Client calls to contact the server for accessing GraphQL NEXT_PUBLIC_PRODUCTION_SERVER = https://blueberryshop.herokuapp.com/graphql # Stripe public key, it doesn't matter if people can see it. NEXT_PUBLIC_STRIPE_TEST_KEY = pk_test_wY7bgBZbTJSklGmANSZbI4bf00cG39wwlu","title":"Environement Variables"},{"location":"3.Client/9.configuration.html#graphql-code-generator","text":"Since all the backend is written in Typescript, it would be great to use those same types in the the frontend. That's exactly what GraphQL Code Generator does, it converts the backend API into Typescript. GraphQL Code Generator needs raw .graphql files to interact with the server. .graphql mutation AddProfilePicture($picture : Upload!) { addProfilePicture(picture : $picture) } GraphQL Code Generator needs a configuration file, named codegen.yaml, it explains to it, how to reach the server and how to retrieve the types. For the project, there are two files, one for development and one for production. GraphQL Code Generator Development overwrite : true schema : ${NEXT_PUBLIC_DEVELOPMENT_SERVER} documents : \"./Graphql/**/**/*.graphql\" # watch: true generates : Graphql/graphql-hooks.ts : plugins : - \"typescript\" - \"typescript-operations\" - \"typescript-react-apollo\" config : withHOC : false withComponent : false withHooks : true GraphQL Code Generator Production overwrite : true schema : ${NEXT_PUBLIC_PRODUCTION_SERVER} documents : \"./Graphql/**/**/*.graphql\" # watch: true generates : Graphql/graphql-hooks.ts : plugins : - \"typescript\" - \"typescript-operations\" - \"typescript-react-apollo\" config : withHOC : false withComponent : false withHooks : true In the package.json file on the client, there is a generate script to execute. The server must be running, otherwise the action will fail. script \"generate:dev\" : \"graphql-codegen -r dotenv/config --config ./codegen.yaml\" , \"generate:prod\" : \"graphql-codegen -r dotenv/config --config ./codegenProd.yaml\" ,","title":"GraphQL Code Generator"},{"location":"3.Client/9.configuration.html#eslint-prettier","text":"Having structured and organized code isn't a nice have, but it's a must. With the help of Eslint, a javascript linter and Prettier, a file formater, it is possible to achieve structured code organization, from solo developer to large teams. Install development dependencies yarn add -D eslint prettier eslint-plugin-prettier eslint-config-prettier eslint-plugin-node eslint-config-node Install Airbnb Style Guide npx install-peerdeps --dev eslint-config-airbnb The Airbnb style guide is a popular set of rules defined by Airbnb for Javascript Create Eslint Configuration Fike eslint --init Eslint Configuration env : browser : true es2021 : true extends : - \"eslint:recommended\" - \"plugin:react/recommended\" - \"plugin:@typescript-eslint/recommended\" - \"airbnb\" parser : \"@typescript-eslint/parser\" parserOptions : ecmaFeatures : jsx : true ecmaVersion : 12 sourceType : module plugins : - react - \"@typescript-eslint\" - \"prettier\" rules : { \"quotes\" : [ \"error\" , \"double\" ], \"no-unused-vars\" : \"warn\" , \"no-console\" : \"off\" , }","title":"Eslint &amp; Prettier"},{"location":"4.Server/1.graphql.html","text":"GraphQL Specification GraphQL is only an API specification, it's server side implementation depends on the GraphQL Specs . Since it is a specification, it can also work well, along REST APIs. No Under & Over Data Fetching In modern web applications, it isn't common to perform multiple requests to a REST API and just use a fraction of the data to display it on the screen. If the data is quiet large and nested, then the longer a user will wait for a response. One of the biggest problems that GraphQL resolves is that it doesn't peform under or over fetching data. It gives you only the data that you asked for. GraphQL Query query { getProducts { products { _id name price description stock promotion status productImages } count } } Autonomous Frontend One other benefit of GraphQL is that it allows the frontend to work autonomously from the backend, since there is just one API endpoint, the backend can freely add resources without impacting the frontend. Documentation Documentation should be treated as code, but often, it is considered as an afterthought. In GraphQL, it is possible to add the documentation directly to the schema and add resource deprecation warnings. Doing so, the documentation can always be up to date and can change easily. Writing a side documentation with the help of Swagger isn't bad, but we, as humans, have a tendency to forget to modify and push the documentation on version control when being on a hurry. Like mentioned, documentation must always be treated as code. Caching A GraphQL request is performed under a HTTP POST request, meaning that a request can't be natively cached without loaders. Facebook, the creator of GraphQL, has also created Dataloader , a generic utility which allows to cache GraphQL requests and send the response in batches. By implementing Dataloader, the GraphQL N+1 query problem can be solved. Libraries Library URI Description Typescript Link Typescript is a superset of Javascript, it allows to add types in development and detect compilation errors at compile time. Apollo-server-express Link Apollo-server-express, not to confuse with apollo-server, integrates very well with an existing Express server and adds GraphQL capabilties. TypeGraphQL Link TypeGraphQL allows to build a Typescript GraphQL API very easily, by just defining resolvers and using decorators, it automaticly creates a GraphQL schema DSL file. Graphql-upload Link Sources Source Author URI GraphQL Official Link How to GraphQL Tutorial Link TypeGraphQL Youtube Playlist Ben Awad Link Upload Files Apollo docs Link Typescript, Next GraphQL Youtube Playlist Ben Awad Link Book Author URI GraphQL in Action Samer Buna Link","title":"GraphQL"},{"location":"4.Server/1.graphql.html#graphql","text":"","title":"GraphQL"},{"location":"4.Server/1.graphql.html#specification","text":"GraphQL is only an API specification, it's server side implementation depends on the GraphQL Specs . Since it is a specification, it can also work well, along REST APIs.","title":"Specification"},{"location":"4.Server/1.graphql.html#no-under-over-data-fetching","text":"In modern web applications, it isn't common to perform multiple requests to a REST API and just use a fraction of the data to display it on the screen. If the data is quiet large and nested, then the longer a user will wait for a response. One of the biggest problems that GraphQL resolves is that it doesn't peform under or over fetching data. It gives you only the data that you asked for. GraphQL Query query { getProducts { products { _id name price description stock promotion status productImages } count } }","title":"No Under &amp; Over Data Fetching"},{"location":"4.Server/1.graphql.html#autonomous-frontend","text":"One other benefit of GraphQL is that it allows the frontend to work autonomously from the backend, since there is just one API endpoint, the backend can freely add resources without impacting the frontend.","title":"Autonomous Frontend"},{"location":"4.Server/1.graphql.html#documentation","text":"Documentation should be treated as code, but often, it is considered as an afterthought. In GraphQL, it is possible to add the documentation directly to the schema and add resource deprecation warnings. Doing so, the documentation can always be up to date and can change easily. Writing a side documentation with the help of Swagger isn't bad, but we, as humans, have a tendency to forget to modify and push the documentation on version control when being on a hurry. Like mentioned, documentation must always be treated as code.","title":"Documentation"},{"location":"4.Server/1.graphql.html#caching","text":"A GraphQL request is performed under a HTTP POST request, meaning that a request can't be natively cached without loaders. Facebook, the creator of GraphQL, has also created Dataloader , a generic utility which allows to cache GraphQL requests and send the response in batches. By implementing Dataloader, the GraphQL N+1 query problem can be solved.","title":"Caching"},{"location":"4.Server/1.graphql.html#libraries","text":"Library URI Description Typescript Link Typescript is a superset of Javascript, it allows to add types in development and detect compilation errors at compile time. Apollo-server-express Link Apollo-server-express, not to confuse with apollo-server, integrates very well with an existing Express server and adds GraphQL capabilties. TypeGraphQL Link TypeGraphQL allows to build a Typescript GraphQL API very easily, by just defining resolvers and using decorators, it automaticly creates a GraphQL schema DSL file. Graphql-upload Link","title":"Libraries"},{"location":"4.Server/1.graphql.html#sources","text":"Source Author URI GraphQL Official Link How to GraphQL Tutorial Link TypeGraphQL Youtube Playlist Ben Awad Link Upload Files Apollo docs Link Typescript, Next GraphQL Youtube Playlist Ben Awad Link Book Author URI GraphQL in Action Samer Buna Link","title":"Sources"},{"location":"4.Server/2.typegoose.html","text":"Typegoose Typegoose is a Typescript wrapper over mongoose for MongoDB. Instead of creating mongoose definitions and then create object interfaces for using Typescript, Typegoose handles this with decorators. Image: Inspired From Michael's Stromer Diagram Typegoose Example // Database import { prop as Property , getModelForClass } from \"@typegoose/typegoose\" ; import { ObjectId } from \"mongodb\" ; // ============================================================================ export class Category { readonly _id : ObjectId ; @Property ({ required : true }) name : string ; } // ============================================================================ export const CategoryModel = getModelForClass ( Category ); Libraries Library URI Typegoose Link","title":"Typegoose"},{"location":"4.Server/2.typegoose.html#typegoose","text":"Typegoose is a Typescript wrapper over mongoose for MongoDB. Instead of creating mongoose definitions and then create object interfaces for using Typescript, Typegoose handles this with decorators. Image: Inspired From Michael's Stromer Diagram Typegoose Example // Database import { prop as Property , getModelForClass } from \"@typegoose/typegoose\" ; import { ObjectId } from \"mongodb\" ; // ============================================================================ export class Category { readonly _id : ObjectId ; @Property ({ required : true }) name : string ; } // ============================================================================ export const CategoryModel = getModelForClass ( Category );","title":"Typegoose"},{"location":"4.Server/2.typegoose.html#libraries","text":"Library URI Typegoose Link","title":"Libraries"},{"location":"4.Server/3.authentication.html","text":"Authentication Authentication is a broad topic, it can be achieved in many ways, like session and cookies, session and in-memory database like Redis, and so on. At the end of the day, authentication is all about trade-offs, control or scalability. Session Authentication Using sessions for authentication gives great control over the service that the application provides to the end user. For obvious reasons, an administrator can create, ban, block or delete a user. Personal Diagram JWT Authentication Json Web Tokens are great for scalability, when using tokens, an extra round trip to the database is saved, improving performance and user experience. The down side of JWT tokens is that when the server emits a token, it has no more control over it. So, if a user violates a rule of conduct, the administrator can't block that user immediately. A way to resolve this, is to add a Time To Live value to the token. Personal Diagram Project Solution Initially, the application was designed to only handle authentication with JWT tokens stored in localStorage. Not only localStorage is vulnerable to XSS attacks but also when Next.js does server side rendering, there is no localStorage API or other storage API available to the server, besides cookies. The solution to this problem was to use both, localStorage API and cookies. Each time that a user calls a page, on server side first, the cookie is sent to the GraphQL authentication middleware. After the page has loaded, transiting from page to page is done via the localstorage API. For the moment this is an hybrid solution, not the most elegant solution, but it works, until a refactor which will fixed the poor design issues of the frontend. Libraries Library URI Description Passport Link Passport is a authentication middleware for Node.js, it allows with the help of strategies to add OAuth capabilities to the application.","title":"Authentication"},{"location":"4.Server/3.authentication.html#authentication","text":"Authentication is a broad topic, it can be achieved in many ways, like session and cookies, session and in-memory database like Redis, and so on. At the end of the day, authentication is all about trade-offs, control or scalability.","title":"Authentication"},{"location":"4.Server/3.authentication.html#session-authentication","text":"Using sessions for authentication gives great control over the service that the application provides to the end user. For obvious reasons, an administrator can create, ban, block or delete a user. Personal Diagram","title":"Session Authentication"},{"location":"4.Server/3.authentication.html#jwt-authentication","text":"Json Web Tokens are great for scalability, when using tokens, an extra round trip to the database is saved, improving performance and user experience. The down side of JWT tokens is that when the server emits a token, it has no more control over it. So, if a user violates a rule of conduct, the administrator can't block that user immediately. A way to resolve this, is to add a Time To Live value to the token. Personal Diagram","title":"JWT Authentication"},{"location":"4.Server/3.authentication.html#project-solution","text":"Initially, the application was designed to only handle authentication with JWT tokens stored in localStorage. Not only localStorage is vulnerable to XSS attacks but also when Next.js does server side rendering, there is no localStorage API or other storage API available to the server, besides cookies. The solution to this problem was to use both, localStorage API and cookies. Each time that a user calls a page, on server side first, the cookie is sent to the GraphQL authentication middleware. After the page has loaded, transiting from page to page is done via the localstorage API. For the moment this is an hybrid solution, not the most elegant solution, but it works, until a refactor which will fixed the poor design issues of the frontend.","title":"Project Solution"},{"location":"4.Server/3.authentication.html#libraries","text":"Library URI Description Passport Link Passport is a authentication middleware for Node.js, it allows with the help of strategies to add OAuth capabilities to the application.","title":"Libraries"},{"location":"4.Server/4.authorization.html","text":"Authorization Authorization Middleware // GraphQL import { MiddlewareFn } from \"type-graphql\" ; import { MyContext } from \"../Graphql/types/MyContext\" ; // ======================================================================= const authorization = ( roles : string []) : MiddlewareFn < MyContext > => async ({ context }, next ) : Promise < void > => { if ( ! roles . includes ( context . req . role )) { await context . res . status ( 400 ). json ({ message : \"You do not have permission to perform this action\" }); } return next (); }; export default authorization ; Example // GraphQL import { UseMiddleware } from \"type-graphql\" ; // Middleware import { authentication } from \"../../../Middleware/authentication\" import authorization from \"../../../Middleware/authorization\" // ================================================================ @UseMiddleware ( authentication ) @UseMiddleware ( authorization ([ \"admin\" ]))","title":"Authorization"},{"location":"4.Server/4.authorization.html#authorization","text":"Authorization Middleware // GraphQL import { MiddlewareFn } from \"type-graphql\" ; import { MyContext } from \"../Graphql/types/MyContext\" ; // ======================================================================= const authorization = ( roles : string []) : MiddlewareFn < MyContext > => async ({ context }, next ) : Promise < void > => { if ( ! roles . includes ( context . req . role )) { await context . res . status ( 400 ). json ({ message : \"You do not have permission to perform this action\" }); } return next (); }; export default authorization ; Example // GraphQL import { UseMiddleware } from \"type-graphql\" ; // Middleware import { authentication } from \"../../../Middleware/authentication\" import authorization from \"../../../Middleware/authorization\" // ================================================================ @UseMiddleware ( authentication ) @UseMiddleware ( authorization ([ \"admin\" ]))","title":"Authorization"},{"location":"4.Server/5.oauth2.html","text":"OAuth2 Image: Inspired From Facebook Docs","title":"OAuth2"},{"location":"4.Server/5.oauth2.html#oauth2","text":"Image: Inspired From Facebook Docs","title":"OAuth2"},{"location":"4.Server/6.payment.html","text":"Payment Stripe Payment Intent Stripe proposes multiple API's for interacting with multiple stripe objects, the simplest one to use for charging clients is the PaymentIntent API. Multiple parties are involved in the process, including the user, the client, the server and Stripe. The user first enters his payment information, next the client_secret is generated from the server, then the users enters his card information and finally Stripe confirms or denies the PaymentIntent. The PaymentIntent API is limited in scope, product information can't be processed. It is still possible to store some custom ID in the metadata field, from there on, via a webhook, it is possible to retrieve all the informations. Image: Inspired From Stripe Documentation Stripe Implementation GraphQL Payment Resolver // GraphQL import { Mutation , Resolver , Arg , Ctx , UseMiddleware } from \"type-graphql\" ; import { StripePaymentIntentInput } from \"./inputs/stripePaymentIntentInput\" ; import { MyContext } from \"src/Graphql/types/MyContext\" ; // Database import { UserModel } from \"@Model/user/User\" ; // Stripe import Stripe from \"stripe\" ; // Middleware import { authentication } from \"@Middleware/authentication\" ; // ================================================================================================= @Resolver () export class CreateStripePaymentIntent { @Mutation (() => String , { nullable : true }) @UseMiddleware ( authentication ) async createStripePaymentIntent ( @Arg ( \"stripePaymentIntent\" ) stripePaymentIntentInput : StripePaymentIntentInput , @Ctx () context : MyContext , ) : Promise < string | null | boolean > { if ( ! context . req . userId ) { return true ; } const user = await UserModel . findOne ({ _id : context.req.userId }); const stripe = new Stripe ( ` ${ process . env . STRIPE_PRIVATE_TEST_KEY } ` , { apiVersion : \"2020-08-27\" , maxNetworkRetries : 1 , timeout : 1000 , }); const paymentIntent = await stripe . paymentIntents . create ({ amount : stripePaymentIntentInput.amount , currency : \"chf\" , description : \"BlueberryShop payment\" , payment_method_types : [ \"card\" ], metadata : { user : ` ${ user ? . _id } ` }, shipping : { name : ` ${ user ? . username } ` , address : { country : stripePaymentIntentInput.shippingCountry , line1 : stripePaymentIntentInput.shippingAddress , city : stripePaymentIntentInput.shippingCity , postal_code : stripePaymentIntentInput.shippingZip , }, }, }); const customer = ` ${ user ? . stripeId } ` ; await stripe . invoiceItems . create ({ customer , amount : 600 , currency : \"chf\" , }); const bla = await stripe . invoices . create ({ customer : ` ${ user ? . stripeId } ` , description : \"hello \" , }); await stripe . invoices . listLineItems ( bla . id ); return paymentIntent . client_secret ; } } Clien Payment Event // Events const onSubmit = async () => { if ( ! stripe || ! elements ) { return ; } const cardElement = elements . getElement ( \"card\" ); const billingDetails = { name : \"Besjan Sejrani\" , email : \"besjan.sejrani@cpnv.ch\" , address : { country : billingCountry , city : billingCity , line1 : billingAddress , postal_code : billingZip , }, }; const paymentMethodReq = await stripe . createPaymentMethod ({ type : \"card\" , card : cardElement , billing_details : billingDetails , }); const { data : clientSecret } = await apolloClient ? . mutate ({ mutation : CreateStripePaymentIntentDocument , variables : { amount : stripeTotal , shippingCountry : checkout (). shippingCountry , shippingAddress : checkout (). shippingAddress , shippingCity : checkout (). shippingCity , shippingZip : checkout (). shippingZip , }, }); await stripe . confirmCardPayment ( clientSecret ? . createStripePaymentIntent , { payment_method : paymentMethodReq.paymentMethod.id , }); await router . push ( \"/admin/checkout/done\" ); }; Webhook Implementation // Express import express from \"express\" ; // Database import { UserModel } from \"@Model/user/User\" ; import { OrderModel } from \"@Model/Order\" ; // UUID import { v4 as uuid } from \"uuid\" ; // Logger import logger from \"@Logger/index\" ; // ======================================================================================================== const router = express . Router (); router . post ( \"/webhook\" , async ( req , res ) => { let paymentIntent ; // Handle the event switch ( req . body . type ) { case \"payment_intent.succeeded\" : paymentIntent = req . body . data . object ; const orderNumber = ` ${ uuid () } ` ; const invoiceNumber = ` ${ uuid () } ` ; // Find corresponding user const user = await UserModel . findById ({ _id : paymentIntent.metadata.user }); const amount = parseInt ( paymentIntent . amount ) / 100 ; // Save user const order = await new OrderModel ({ fullName : ` ${ user ? . firstName } ${ user ? . lastName } ` , amount , cart : user?.cart , billing : { address : paymentIntent.charges.data [ 0 ]. billing_details . address . line1 , city : paymentIntent.charges.data [ 0 ]. billing_details . address . city , zip : paymentIntent.charges.data [ 0 ]. billing_details . address . postal_code , country : paymentIntent.charges.data [ 0 ]. billing_details . address . country , }, shipping : { address : paymentIntent.shipping.address.line1 , city : paymentIntent.shipping.address.city , zip : paymentIntent.shipping.address.postal_code , country : paymentIntent.shipping.address.country , }, orderNumber , invoiceNumber , }); const OrderInformation = await order . save (); await logger . info ( `Successful Order, order _id: ${ OrderInformation . _id } ` ); // Delete user cart await UserModel . findOneAndUpdate ( { _id : paymentIntent.metadata.user }, { $pull : { cart : { $exists : true }, }, }, ); break ; case \"payment_intent.payment_failed\" : paymentIntent = req . body . data . object ; console . log ( \"PaymentMethod was attached to a Customer!\" , paymentIntent ); await logger . info ( `Order Failed, order id: [ ${ paymentIntent . id } ]` ); break ; default : console . log ( `Unhandled event type ${ req . body . type } ` ); } res . status ( 200 ). json ({ received : true }); }); export default router ; Webhooks Stripe webhooks work only with the HTTPS protocol, therefore, the stripe command line interface must be used in development environments. After installing and launching the CLI, a port and a path endpoint must be specified so that Stripe can send the webhook request. Specific Stripe trigger events can be found here . Listen to project port and webhook route stripe listen --forward-to localhost:4000/webhook Trigger Action stripe trigger payment_intent.succeeded Install Stripe CLI homebrew brew install stripe/stripe-cli/stripe macOS # Download tar.gz file on https://github.com/stripe/stripe-cli/releases/latest tar -xvf stripe_X.X.X_mac-os_x86_64.tar.gz # Execute /usr/local/bin Linux # Download tar.gz file on https://github.com/stripe/stripe-cli/releases/latest -xvf stripe_X.X.X_linux_x86_64.tar.gz # Execute ./stripe Windows # Download tar.gz file on https://github.com/stripe/stripe-cli/releases/latest stripe_X.X.X_windows_x86_64.zip # Execute .exe Docker docker run --rm -it stripe/stripe-cli:latest Stripe Testing Cards Stripe provides multiple test cards during development. Feel free to check their official documentation. Card Number Brand CVC Date 4242 4242 4242 4242 Visa Any 3 digits Any future date 4000 0566 5566 5556 Visa(debit) Any 3 digits Any future date 5555 5555 5555 4444 MasterCard Any 3 digits Any future date ... ... ... ... Sources Source Author URI Playlist react-stripe-js Stripe Developpers Link Playlist stripe-node Stripe Developpers Link Stripe Demo Donut Stripe Thomas Marek Link Stripe API Stripe Documentation Link Stripe CLI Stripe Documentation Link Stripe Trigger Events Stripe Documentation Link Stripe Testing Cards Stripe Documentation Link","title":"Payment"},{"location":"4.Server/6.payment.html#payment","text":"","title":"Payment"},{"location":"4.Server/6.payment.html#stripe-payment-intent","text":"Stripe proposes multiple API's for interacting with multiple stripe objects, the simplest one to use for charging clients is the PaymentIntent API. Multiple parties are involved in the process, including the user, the client, the server and Stripe. The user first enters his payment information, next the client_secret is generated from the server, then the users enters his card information and finally Stripe confirms or denies the PaymentIntent. The PaymentIntent API is limited in scope, product information can't be processed. It is still possible to store some custom ID in the metadata field, from there on, via a webhook, it is possible to retrieve all the informations. Image: Inspired From Stripe Documentation","title":"Stripe Payment Intent"},{"location":"4.Server/6.payment.html#stripe-implementation","text":"GraphQL Payment Resolver // GraphQL import { Mutation , Resolver , Arg , Ctx , UseMiddleware } from \"type-graphql\" ; import { StripePaymentIntentInput } from \"./inputs/stripePaymentIntentInput\" ; import { MyContext } from \"src/Graphql/types/MyContext\" ; // Database import { UserModel } from \"@Model/user/User\" ; // Stripe import Stripe from \"stripe\" ; // Middleware import { authentication } from \"@Middleware/authentication\" ; // ================================================================================================= @Resolver () export class CreateStripePaymentIntent { @Mutation (() => String , { nullable : true }) @UseMiddleware ( authentication ) async createStripePaymentIntent ( @Arg ( \"stripePaymentIntent\" ) stripePaymentIntentInput : StripePaymentIntentInput , @Ctx () context : MyContext , ) : Promise < string | null | boolean > { if ( ! context . req . userId ) { return true ; } const user = await UserModel . findOne ({ _id : context.req.userId }); const stripe = new Stripe ( ` ${ process . env . STRIPE_PRIVATE_TEST_KEY } ` , { apiVersion : \"2020-08-27\" , maxNetworkRetries : 1 , timeout : 1000 , }); const paymentIntent = await stripe . paymentIntents . create ({ amount : stripePaymentIntentInput.amount , currency : \"chf\" , description : \"BlueberryShop payment\" , payment_method_types : [ \"card\" ], metadata : { user : ` ${ user ? . _id } ` }, shipping : { name : ` ${ user ? . username } ` , address : { country : stripePaymentIntentInput.shippingCountry , line1 : stripePaymentIntentInput.shippingAddress , city : stripePaymentIntentInput.shippingCity , postal_code : stripePaymentIntentInput.shippingZip , }, }, }); const customer = ` ${ user ? . stripeId } ` ; await stripe . invoiceItems . create ({ customer , amount : 600 , currency : \"chf\" , }); const bla = await stripe . invoices . create ({ customer : ` ${ user ? . stripeId } ` , description : \"hello \" , }); await stripe . invoices . listLineItems ( bla . id ); return paymentIntent . client_secret ; } } Clien Payment Event // Events const onSubmit = async () => { if ( ! stripe || ! elements ) { return ; } const cardElement = elements . getElement ( \"card\" ); const billingDetails = { name : \"Besjan Sejrani\" , email : \"besjan.sejrani@cpnv.ch\" , address : { country : billingCountry , city : billingCity , line1 : billingAddress , postal_code : billingZip , }, }; const paymentMethodReq = await stripe . createPaymentMethod ({ type : \"card\" , card : cardElement , billing_details : billingDetails , }); const { data : clientSecret } = await apolloClient ? . mutate ({ mutation : CreateStripePaymentIntentDocument , variables : { amount : stripeTotal , shippingCountry : checkout (). shippingCountry , shippingAddress : checkout (). shippingAddress , shippingCity : checkout (). shippingCity , shippingZip : checkout (). shippingZip , }, }); await stripe . confirmCardPayment ( clientSecret ? . createStripePaymentIntent , { payment_method : paymentMethodReq.paymentMethod.id , }); await router . push ( \"/admin/checkout/done\" ); }; Webhook Implementation // Express import express from \"express\" ; // Database import { UserModel } from \"@Model/user/User\" ; import { OrderModel } from \"@Model/Order\" ; // UUID import { v4 as uuid } from \"uuid\" ; // Logger import logger from \"@Logger/index\" ; // ======================================================================================================== const router = express . Router (); router . post ( \"/webhook\" , async ( req , res ) => { let paymentIntent ; // Handle the event switch ( req . body . type ) { case \"payment_intent.succeeded\" : paymentIntent = req . body . data . object ; const orderNumber = ` ${ uuid () } ` ; const invoiceNumber = ` ${ uuid () } ` ; // Find corresponding user const user = await UserModel . findById ({ _id : paymentIntent.metadata.user }); const amount = parseInt ( paymentIntent . amount ) / 100 ; // Save user const order = await new OrderModel ({ fullName : ` ${ user ? . firstName } ${ user ? . lastName } ` , amount , cart : user?.cart , billing : { address : paymentIntent.charges.data [ 0 ]. billing_details . address . line1 , city : paymentIntent.charges.data [ 0 ]. billing_details . address . city , zip : paymentIntent.charges.data [ 0 ]. billing_details . address . postal_code , country : paymentIntent.charges.data [ 0 ]. billing_details . address . country , }, shipping : { address : paymentIntent.shipping.address.line1 , city : paymentIntent.shipping.address.city , zip : paymentIntent.shipping.address.postal_code , country : paymentIntent.shipping.address.country , }, orderNumber , invoiceNumber , }); const OrderInformation = await order . save (); await logger . info ( `Successful Order, order _id: ${ OrderInformation . _id } ` ); // Delete user cart await UserModel . findOneAndUpdate ( { _id : paymentIntent.metadata.user }, { $pull : { cart : { $exists : true }, }, }, ); break ; case \"payment_intent.payment_failed\" : paymentIntent = req . body . data . object ; console . log ( \"PaymentMethod was attached to a Customer!\" , paymentIntent ); await logger . info ( `Order Failed, order id: [ ${ paymentIntent . id } ]` ); break ; default : console . log ( `Unhandled event type ${ req . body . type } ` ); } res . status ( 200 ). json ({ received : true }); }); export default router ;","title":"Stripe Implementation"},{"location":"4.Server/6.payment.html#webhooks","text":"Stripe webhooks work only with the HTTPS protocol, therefore, the stripe command line interface must be used in development environments. After installing and launching the CLI, a port and a path endpoint must be specified so that Stripe can send the webhook request. Specific Stripe trigger events can be found here . Listen to project port and webhook route stripe listen --forward-to localhost:4000/webhook Trigger Action stripe trigger payment_intent.succeeded","title":"Webhooks"},{"location":"4.Server/6.payment.html#install-stripe-cli","text":"homebrew brew install stripe/stripe-cli/stripe macOS # Download tar.gz file on https://github.com/stripe/stripe-cli/releases/latest tar -xvf stripe_X.X.X_mac-os_x86_64.tar.gz # Execute /usr/local/bin Linux # Download tar.gz file on https://github.com/stripe/stripe-cli/releases/latest -xvf stripe_X.X.X_linux_x86_64.tar.gz # Execute ./stripe Windows # Download tar.gz file on https://github.com/stripe/stripe-cli/releases/latest stripe_X.X.X_windows_x86_64.zip # Execute .exe Docker docker run --rm -it stripe/stripe-cli:latest","title":"Install Stripe CLI"},{"location":"4.Server/6.payment.html#stripe-testing-cards","text":"Stripe provides multiple test cards during development. Feel free to check their official documentation. Card Number Brand CVC Date 4242 4242 4242 4242 Visa Any 3 digits Any future date 4000 0566 5566 5556 Visa(debit) Any 3 digits Any future date 5555 5555 5555 4444 MasterCard Any 3 digits Any future date ... ... ... ...","title":"Stripe Testing Cards"},{"location":"4.Server/6.payment.html#sources","text":"Source Author URI Playlist react-stripe-js Stripe Developpers Link Playlist stripe-node Stripe Developpers Link Stripe Demo Donut Stripe Thomas Marek Link Stripe API Stripe Documentation Link Stripe CLI Stripe Documentation Link Stripe Trigger Events Stripe Documentation Link Stripe Testing Cards Stripe Documentation Link","title":"Sources"},{"location":"4.Server/7.upload.html","text":"File Upload AWS S3 By importing the graphql-upload package, a new scalar type, GraphQLUpload, will become available. It allows to redirect an incoming stream to an output stream, from there on, the stream is redirected to a AWS S3 bucket. GraphQLUpload Scalar // Upload import { GraphQLUpload } from \"graphql-upload\" ; import { Upload } from \"../../types/Upload\" ; // ========================================================== async createProduct ( @Arg ( \"picture\" , () => [ GraphQLUpload ]) FileList : Upload [], @Arg ( \"input\" ) { name , price , description , stock , category , promotion , status } : CreateProductInput ) : Promise < any > Upload Type import { Stream } from \"stream\" ; export interface Upload { filename : string ; mimetype : string ; encoding : string ; createReadStream : () => Stream ; } export interface UploadedFileResponse { filename : string ; mimetype : string ; encoding : string ; url : string ; } export interface IUploader { singleFileUploadResolver : ({ file } : { file : File }) => Promise < UploadedFileResponse > ; multipleUploadsResolver : ({ files } : { files : File [] }) => Promise < UploadedFileResponse [] > ; } After the scalar type is available, the next action will be to interact with the aws-sdk and use their API for basic CRUD actions. S3 Class type S3Type = { accessKeyId : string ; secretAccessKey : string ; signatureVersion : string ; region : string ; bucket : string ; }; type S3UploadStream = { writeStream : stream.PassThrough ; promise : Promise < AWS . S3 . ManagedUpload . SendData > ; }; export class S3 { private s3 : AWS.S3 ; public s3Config : S3Type ; constructor ( config : S3Type ) { AWS . config = new AWS . Config (); AWS . config . update ({ region : config.region , accessKeyId : config.accessKeyId , signatureVersion : config.signatureVersion , secretAccessKey : config.secretAccessKey , }); this . s3 = new AWS . S3 (); this . s3Config = config ; } private createUploadStream ( key : string ) : S3UploadStream { const pass = new stream . PassThrough (); return { writeStream : pass , promise : this.s3 . upload ({ Bucket : this.s3Config.bucket , Key : key , Body : pass , }) . promise (), }; } private createDestinationFilePath ( fileName : string , _mimetype? : string , _encoding? : string ) : string { return fileName ; } Single upload async singleFileUploadResolver ({ file , } : { file : ApolloServerFileUploads.Upload ; }) : Promise < ApolloServerFileUploads . UploadedFileResponse > { const { createReadStream , filename , mimetype , encoding } = await file ; const extension = filename . split ( \".\" )[ 1 ]; const newFileName = ` ${ uuid () } . ${ extension } ` ; const filePath = this . createDestinationFilePath ( newFileName , mimetype , encoding ); const uploadStream = this . createUploadStream ( filePath ); createReadStream (). pipe ( uploadStream . writeStream ); const result = await uploadStream . promise ; return await { filename , mimetype , encoding , url : result.Location }; } Multiple uploads async multipleUploads ({ files , } : { files : ApolloServerFileUploads.Upload []; }) : Promise < ApolloServerFileUploads . UploadedFileResponse [] > { return Promise . all ( files . map (( f ) => this . singleFileUploadResolver ({ file : f }))); } Delete image async deleteProductImage ( key : string ) { this . s3 . deleteObject ( { Bucket : process.env.AMAZON_S3_BUCKET , Key : key , }, function ( err , _data ) { if ( err ) { console . log ( err ); } } ); } Delete images async deleteMultipleImages ( keys = []) { const objects = keys . map (( key ) => ({ Key : key })); const params = { Bucket : process.env.AMAZON_S3_BUCKET , Delete : { Objects : objects }, }; this . s3 . deleteObjects ( params ). promise (); } Libraries Library URI Description Graphql-upload Link","title":"File Upload"},{"location":"4.Server/7.upload.html#file-upload","text":"","title":"File Upload"},{"location":"4.Server/7.upload.html#aws-s3","text":"By importing the graphql-upload package, a new scalar type, GraphQLUpload, will become available. It allows to redirect an incoming stream to an output stream, from there on, the stream is redirected to a AWS S3 bucket. GraphQLUpload Scalar // Upload import { GraphQLUpload } from \"graphql-upload\" ; import { Upload } from \"../../types/Upload\" ; // ========================================================== async createProduct ( @Arg ( \"picture\" , () => [ GraphQLUpload ]) FileList : Upload [], @Arg ( \"input\" ) { name , price , description , stock , category , promotion , status } : CreateProductInput ) : Promise < any > Upload Type import { Stream } from \"stream\" ; export interface Upload { filename : string ; mimetype : string ; encoding : string ; createReadStream : () => Stream ; } export interface UploadedFileResponse { filename : string ; mimetype : string ; encoding : string ; url : string ; } export interface IUploader { singleFileUploadResolver : ({ file } : { file : File }) => Promise < UploadedFileResponse > ; multipleUploadsResolver : ({ files } : { files : File [] }) => Promise < UploadedFileResponse [] > ; } After the scalar type is available, the next action will be to interact with the aws-sdk and use their API for basic CRUD actions. S3 Class type S3Type = { accessKeyId : string ; secretAccessKey : string ; signatureVersion : string ; region : string ; bucket : string ; }; type S3UploadStream = { writeStream : stream.PassThrough ; promise : Promise < AWS . S3 . ManagedUpload . SendData > ; }; export class S3 { private s3 : AWS.S3 ; public s3Config : S3Type ; constructor ( config : S3Type ) { AWS . config = new AWS . Config (); AWS . config . update ({ region : config.region , accessKeyId : config.accessKeyId , signatureVersion : config.signatureVersion , secretAccessKey : config.secretAccessKey , }); this . s3 = new AWS . S3 (); this . s3Config = config ; } private createUploadStream ( key : string ) : S3UploadStream { const pass = new stream . PassThrough (); return { writeStream : pass , promise : this.s3 . upload ({ Bucket : this.s3Config.bucket , Key : key , Body : pass , }) . promise (), }; } private createDestinationFilePath ( fileName : string , _mimetype? : string , _encoding? : string ) : string { return fileName ; } Single upload async singleFileUploadResolver ({ file , } : { file : ApolloServerFileUploads.Upload ; }) : Promise < ApolloServerFileUploads . UploadedFileResponse > { const { createReadStream , filename , mimetype , encoding } = await file ; const extension = filename . split ( \".\" )[ 1 ]; const newFileName = ` ${ uuid () } . ${ extension } ` ; const filePath = this . createDestinationFilePath ( newFileName , mimetype , encoding ); const uploadStream = this . createUploadStream ( filePath ); createReadStream (). pipe ( uploadStream . writeStream ); const result = await uploadStream . promise ; return await { filename , mimetype , encoding , url : result.Location }; } Multiple uploads async multipleUploads ({ files , } : { files : ApolloServerFileUploads.Upload []; }) : Promise < ApolloServerFileUploads . UploadedFileResponse [] > { return Promise . all ( files . map (( f ) => this . singleFileUploadResolver ({ file : f }))); } Delete image async deleteProductImage ( key : string ) { this . s3 . deleteObject ( { Bucket : process.env.AMAZON_S3_BUCKET , Key : key , }, function ( err , _data ) { if ( err ) { console . log ( err ); } } ); } Delete images async deleteMultipleImages ( keys = []) { const objects = keys . map (( key ) => ({ Key : key })); const params = { Bucket : process.env.AMAZON_S3_BUCKET , Delete : { Objects : objects }, }; this . s3 . deleteObjects ( params ). promise (); }","title":"AWS S3"},{"location":"4.Server/7.upload.html#libraries","text":"Library URI Description Graphql-upload Link","title":"Libraries"},{"location":"4.Server/8.testing.html","text":"Testing Warning For the moment, testing isn't fully implemented in the project, work in progress Unit Tests Coming soon. Integration Tests Code coverage Coming soon. Table Tests Coming soon. Jest Configuration Configuration file // TS Paths const getJestMappersFromTSConfig = require ( \"tsconfig-paths-jest-mapper\" ); const module NameMapper = getJestMappersFromTSConfig (); // ======================================================================== module .exports = { preset : \"ts-jest\" , testEnvironment : \"node\" , setupFilesAfterEnv : [ \"<rootDir>/src/utils/test/setup.ts\" ], module NameMapper , }; Two ways of testing Testing a project can be done in two different ways, it depends on the perspective. Doing a lot of unit tests in the application is easy to implement and gives the confidence that each tested function / method applies the separation of concern rule. The downside is that it doesn't give much information about how the system, as a all, behaves. Further integration tests and end-to-end tests are required. Image: Personal Diagram In reverse, a lot of end-to-end tests can be implemented in contrast to unit tests, they help to immediately show some results. The downside is that the lack of unit testing can be the source of issues. Image: Personal Diagram Libraries Library URI Description Jest Link Jest is a testing library built by Facebook. It is a test runner providing data mocking, code coverage and snapshots. Jest is great for testing units tests and integration tests. Ts-Jest Link Ts-Jest is a Jest trasnformer which allows to test Typescript projects","title":"Testing"},{"location":"4.Server/8.testing.html#testing","text":"Warning For the moment, testing isn't fully implemented in the project, work in progress","title":"Testing"},{"location":"4.Server/8.testing.html#unit-tests","text":"Coming soon.","title":"Unit Tests"},{"location":"4.Server/8.testing.html#integration-tests","text":"","title":"Integration Tests"},{"location":"4.Server/8.testing.html#code-coverage","text":"Coming soon.","title":"Code coverage"},{"location":"4.Server/8.testing.html#table-tests","text":"Coming soon.","title":"Table Tests"},{"location":"4.Server/8.testing.html#jest-configuration","text":"Configuration file // TS Paths const getJestMappersFromTSConfig = require ( \"tsconfig-paths-jest-mapper\" ); const module NameMapper = getJestMappersFromTSConfig (); // ======================================================================== module .exports = { preset : \"ts-jest\" , testEnvironment : \"node\" , setupFilesAfterEnv : [ \"<rootDir>/src/utils/test/setup.ts\" ], module NameMapper , };","title":"Jest Configuration"},{"location":"4.Server/8.testing.html#two-ways-of-testing","text":"Testing a project can be done in two different ways, it depends on the perspective. Doing a lot of unit tests in the application is easy to implement and gives the confidence that each tested function / method applies the separation of concern rule. The downside is that it doesn't give much information about how the system, as a all, behaves. Further integration tests and end-to-end tests are required. Image: Personal Diagram In reverse, a lot of end-to-end tests can be implemented in contrast to unit tests, they help to immediately show some results. The downside is that the lack of unit testing can be the source of issues. Image: Personal Diagram","title":"Two ways of testing"},{"location":"4.Server/8.testing.html#libraries","text":"Library URI Description Jest Link Jest is a testing library built by Facebook. It is a test runner providing data mocking, code coverage and snapshots. Jest is great for testing units tests and integration tests. Ts-Jest Link Ts-Jest is a Jest trasnformer which allows to test Typescript projects","title":"Libraries"},{"location":"4.Server/9.errorhandling.html","text":"Error Handling Warning For the moment, error handling isn't fully implemented in the project, work in progress Async await without try/catch Despite the great evolution in recent years from callback hell to async await, writing asynchronous function with try/catch feels clumsy, not to mention the loss of readiness. After trying out the Go programming language, new ways of handling errors where discovered. Errors are immediately checked, and not delayed to the catch statement. Although it can can be considered as a redundant task to always check returned values, it greatly improves code readiness. to() export default function to ( promise ) { return promise . then (( data ) => { return [ null , data ]; }) . catch (( err : Error ) => [ err ]); } to() Example // Error Handling import to from \"@Error/to\" ; // ============================================================================== const [ err , data ] = await to ( UserModel . findById ( decoded . _id )); if ( err ) return context . res . status ( 400 ). json ({ message : \"You must be loged in\" }); const user = data ; Custom Errors Custom errors where defined from the Error class, each one responds to a specific HTTP status code. Error Status Code CustomError - BadRequestError 400 NotAuthorizedError 401 NotFoundError 404 DatabaseConnectionError 500 Sources Source Author URI An alternative way to use async/await without try/catch blocks in Node JS Rog\u00e9rio de Oliveira Link How to write async await without try-catch blocks in Javascript Dima Grossman Link","title":"Error Handling"},{"location":"4.Server/9.errorhandling.html#error-handling","text":"Warning For the moment, error handling isn't fully implemented in the project, work in progress","title":"Error Handling"},{"location":"4.Server/9.errorhandling.html#async-await-without-trycatch","text":"Despite the great evolution in recent years from callback hell to async await, writing asynchronous function with try/catch feels clumsy, not to mention the loss of readiness. After trying out the Go programming language, new ways of handling errors where discovered. Errors are immediately checked, and not delayed to the catch statement. Although it can can be considered as a redundant task to always check returned values, it greatly improves code readiness. to() export default function to ( promise ) { return promise . then (( data ) => { return [ null , data ]; }) . catch (( err : Error ) => [ err ]); } to() Example // Error Handling import to from \"@Error/to\" ; // ============================================================================== const [ err , data ] = await to ( UserModel . findById ( decoded . _id )); if ( err ) return context . res . status ( 400 ). json ({ message : \"You must be loged in\" }); const user = data ;","title":"Async await without try/catch"},{"location":"4.Server/9.errorhandling.html#custom-errors","text":"Custom errors where defined from the Error class, each one responds to a specific HTTP status code. Error Status Code CustomError - BadRequestError 400 NotAuthorizedError 401 NotFoundError 404 DatabaseConnectionError 500","title":"Custom Errors"},{"location":"4.Server/9.errorhandling.html#sources","text":"Source Author URI An alternative way to use async/await without try/catch blocks in Node JS Rog\u00e9rio de Oliveira Link How to write async await without try-catch blocks in Javascript Dima Grossman Link","title":"Sources"},{"location":"4.Server/a.logging.html","text":"Logging Warning For the moment, logging isn't fully implemented in the project, work in progress Production & Development Logs In development, logs are defined in plain text, as for production, they are defined in Json. Production Logs Example {\"message\":\"blaaa\",\"level\":\"\\u001b[31merror\\u001b[39m\",\"service\":\"server-service\",\"timestamp\":\"2021-04-30T08:39:46.421Z\"} {\"message\":\"blaaa\",\"level\":\"\\u001b[33mwarn\\u001b[39m\",\"service\":\"server-service\",\"timestamp\":\"2021-04-30T08:39:46.422Z\"} Devlopment Logs Example 2021-04-30 10:30:28 error: blaaa 2021-04-30 10:30:28 warn: blaaa 2021-04-30 10:30:28 info: blaaa Log Levels The Winston logger has different levels of logging, from zero, the most important, to six, the least important. Log Type Log Level error 0 warn 1 info 2 http 3 verbose 4 debug 5 silly 6 Log Format Logs should be parsable and scannable, they can be used in a wide range of fields like tracing, analytics or audits. Using brackets around log values can help to improve readiness, it also helps for searching text across regular expression or by using the seed Unix command line utility. Example [2021-05-23 14:25:43] [info]: Order Failed, order id: [pi_1IuIFNGuKRma2QlhhnOTARM5] Sources Source Author URI Winston Documentation Github Link Logging Best Practices Dave McAllister Link Presentation Logging Joseph Reeve Link","title":"Logging"},{"location":"4.Server/a.logging.html#logging","text":"Warning For the moment, logging isn't fully implemented in the project, work in progress","title":"Logging"},{"location":"4.Server/a.logging.html#production-development-logs","text":"In development, logs are defined in plain text, as for production, they are defined in Json. Production Logs Example {\"message\":\"blaaa\",\"level\":\"\\u001b[31merror\\u001b[39m\",\"service\":\"server-service\",\"timestamp\":\"2021-04-30T08:39:46.421Z\"} {\"message\":\"blaaa\",\"level\":\"\\u001b[33mwarn\\u001b[39m\",\"service\":\"server-service\",\"timestamp\":\"2021-04-30T08:39:46.422Z\"} Devlopment Logs Example 2021-04-30 10:30:28 error: blaaa 2021-04-30 10:30:28 warn: blaaa 2021-04-30 10:30:28 info: blaaa","title":"Production &amp; Development Logs"},{"location":"4.Server/a.logging.html#log-levels","text":"The Winston logger has different levels of logging, from zero, the most important, to six, the least important. Log Type Log Level error 0 warn 1 info 2 http 3 verbose 4 debug 5 silly 6","title":"Log Levels"},{"location":"4.Server/a.logging.html#log-format","text":"Logs should be parsable and scannable, they can be used in a wide range of fields like tracing, analytics or audits. Using brackets around log values can help to improve readiness, it also helps for searching text across regular expression or by using the seed Unix command line utility. Example [2021-05-23 14:25:43] [info]: Order Failed, order id: [pi_1IuIFNGuKRma2QlhhnOTARM5]","title":"Log Format"},{"location":"4.Server/a.logging.html#sources","text":"Source Author URI Winston Documentation Github Link Logging Best Practices Dave McAllister Link Presentation Logging Joseph Reeve Link","title":"Sources"},{"location":"4.Server/b.configuration.html","text":"Configuration Environment Variables Warning Environment Variables are sensible information injected just before a server is starting. Don't forget to add the environment variable file to .gitignore, otherwise bots will scrape GitHub for sensible information and use your aws credentials, for example, for mining Bitcoin. .env # You can chose another port than 4000, but changes must also be made in the client .env file PORT = 4000 # MongoDB Atlas credentials, change the values in UPPERCASE MONGO_ATLAS = mongodb+srv://USER:PASSWORD@HOST/DATABASE?retryWrites = true & w = majority # Can stay as it is, used only with docker-compose in development MONGO_PRODUCTION = mongodb://api_user:api1234@mongodb/api_prod_db?authSource = api_prod_db & readPreference = primary & appname = MongoDB%20Compass & ssl = false MONGO_DEVELOPMENT = mongodb://api_user:api1234@mongodb/api_dev_db?authSource = api_dev_db & readPreference = primary & appname = MongoDB%20Compass & ssl = false # Can stay as it is, used only with docker-compose in development MONGO_TEST = mongodb://api_user:api1234@mongodb/api_test_db?authSource = api_test_db & readPreference = primary & appname = MongoDB%20Compass & ssl = false MONGO_TEST_JEST = mongodb://api_user:api1234@localhost:27017/api_test_db?authSource = api_test_db & readPreference = primary & appname = MongoDB%20Compass & ssl = false # Define any value for the JWT_SECRET JWT_SECRET = # Define expiration in milliseconds or in days JWT_EXPIRES_IN = # Cross-Origin Resource Sharing configuration value, http://localhost:3000 CORS_DOMAIN = # Cookies bound to a specific domain, localhost COOKIES_DOMAIN = # Define your Sendgrid secret API key SENDGRID_API = # Redis credentials are required for using it as a secondary database for TTL values REDIS_HOST = REDIS_PORT = REDIS_PASSWORD = REDIS_URL = # Github credentials are required for OAuth2 GITHUB_CLIENT_ID = GITHUB_CLIENT_SECRET = # Google credentials are required for OAuth2 GOOGLE_ID = GOOGLE_SECRET = # Twitter credentials are required for OAuth2 TWITTER_API_KEY = TWITTER_API_SECRET_KEY = TWITTER_BEARER_TOKEN = # Amazon S3 bucket name and Amazon credentials are required for storing images on S3 AMAZON_S3_BUCKET = AMAZON_KEY_ID = AMAZON_SECRET_ACCESS_KEY = # Stripe test credentials are required for the development environment STRIPE_PUBLIC_TEST_KEY = STRIPE_PRIVATE_TEST_KEY = # Log level defined and above # error 0 # warn 1 # info 2 # http 3 # verbose 4 # debug 5 # silly 6 LOG_LEVEL_PPRODUCTION = warn LOG_LEVEL_DEVELOPMENT = info Eslint & Prettier Having structured and organized code isn't a nice have, but it's a must. With the help of Eslint, a javascript linter and Prettier, a file formater, it is possible to achieve structured code organization, from solo developer to large teams. Install development dependencies yarn add -D eslint prettier eslint-plugin-prettier eslint-config-prettier eslint-plugin-node eslint-config-node Install Airbnb Style Guide npx install-peerdeps --dev eslint-config-airbnb The Airbnb style guide is a popular set of rules defined by Airbnb for Javascript Create Eslint Configuration Fike eslint --init Eslint Configuration env : browser : true es2021 : true extends : - \"eslint:recommended\" - \"plugin:@typescript-eslint/recommended\" - \"airbnb\" parser : \"@typescript-eslint/parser\" parserOptions : ecmaVersion : 12 sourceType : module plugins : - \"@typescript-eslint\" - \"prettier\" rules : { \"quotes\" : [ \"error\" , \"double\" ], \"no-unused-vars\" : \"warn\" , \"no-console\" : \"off\" , }","title":"Configuration"},{"location":"4.Server/b.configuration.html#configuration","text":"","title":"Configuration"},{"location":"4.Server/b.configuration.html#environment-variables","text":"Warning Environment Variables are sensible information injected just before a server is starting. Don't forget to add the environment variable file to .gitignore, otherwise bots will scrape GitHub for sensible information and use your aws credentials, for example, for mining Bitcoin. .env # You can chose another port than 4000, but changes must also be made in the client .env file PORT = 4000 # MongoDB Atlas credentials, change the values in UPPERCASE MONGO_ATLAS = mongodb+srv://USER:PASSWORD@HOST/DATABASE?retryWrites = true & w = majority # Can stay as it is, used only with docker-compose in development MONGO_PRODUCTION = mongodb://api_user:api1234@mongodb/api_prod_db?authSource = api_prod_db & readPreference = primary & appname = MongoDB%20Compass & ssl = false MONGO_DEVELOPMENT = mongodb://api_user:api1234@mongodb/api_dev_db?authSource = api_dev_db & readPreference = primary & appname = MongoDB%20Compass & ssl = false # Can stay as it is, used only with docker-compose in development MONGO_TEST = mongodb://api_user:api1234@mongodb/api_test_db?authSource = api_test_db & readPreference = primary & appname = MongoDB%20Compass & ssl = false MONGO_TEST_JEST = mongodb://api_user:api1234@localhost:27017/api_test_db?authSource = api_test_db & readPreference = primary & appname = MongoDB%20Compass & ssl = false # Define any value for the JWT_SECRET JWT_SECRET = # Define expiration in milliseconds or in days JWT_EXPIRES_IN = # Cross-Origin Resource Sharing configuration value, http://localhost:3000 CORS_DOMAIN = # Cookies bound to a specific domain, localhost COOKIES_DOMAIN = # Define your Sendgrid secret API key SENDGRID_API = # Redis credentials are required for using it as a secondary database for TTL values REDIS_HOST = REDIS_PORT = REDIS_PASSWORD = REDIS_URL = # Github credentials are required for OAuth2 GITHUB_CLIENT_ID = GITHUB_CLIENT_SECRET = # Google credentials are required for OAuth2 GOOGLE_ID = GOOGLE_SECRET = # Twitter credentials are required for OAuth2 TWITTER_API_KEY = TWITTER_API_SECRET_KEY = TWITTER_BEARER_TOKEN = # Amazon S3 bucket name and Amazon credentials are required for storing images on S3 AMAZON_S3_BUCKET = AMAZON_KEY_ID = AMAZON_SECRET_ACCESS_KEY = # Stripe test credentials are required for the development environment STRIPE_PUBLIC_TEST_KEY = STRIPE_PRIVATE_TEST_KEY = # Log level defined and above # error 0 # warn 1 # info 2 # http 3 # verbose 4 # debug 5 # silly 6 LOG_LEVEL_PPRODUCTION = warn LOG_LEVEL_DEVELOPMENT = info","title":"Environment Variables"},{"location":"4.Server/b.configuration.html#eslint-prettier","text":"Having structured and organized code isn't a nice have, but it's a must. With the help of Eslint, a javascript linter and Prettier, a file formater, it is possible to achieve structured code organization, from solo developer to large teams. Install development dependencies yarn add -D eslint prettier eslint-plugin-prettier eslint-config-prettier eslint-plugin-node eslint-config-node Install Airbnb Style Guide npx install-peerdeps --dev eslint-config-airbnb The Airbnb style guide is a popular set of rules defined by Airbnb for Javascript Create Eslint Configuration Fike eslint --init Eslint Configuration env : browser : true es2021 : true extends : - \"eslint:recommended\" - \"plugin:@typescript-eslint/recommended\" - \"airbnb\" parser : \"@typescript-eslint/parser\" parserOptions : ecmaVersion : 12 sourceType : module plugins : - \"@typescript-eslint\" - \"prettier\" rules : { \"quotes\" : [ \"error\" , \"double\" ], \"no-unused-vars\" : \"warn\" , \"no-console\" : \"off\" , }","title":"Eslint &amp; Prettier"},{"location":"4.Server/c.problems.html","text":"Problems Time To Live MongoDB The forgot password functionality relies on a specific TTL value to be executed. In MongoDB it is possible to specify a an expiration time, but it deletes the entire document and not just an individual field. To resolve this issue, Redis will be used instead. Redis feels just to be the right tool for the right job. Heroku offers a free tier, it's plenty enough. Typegoose @ Property ({ expires : \"30s\" , default : Date . now () }) changePassword : Date ;","title":"Problems"},{"location":"4.Server/c.problems.html#problems","text":"","title":"Problems"},{"location":"4.Server/c.problems.html#time-to-live-mongodb","text":"The forgot password functionality relies on a specific TTL value to be executed. In MongoDB it is possible to specify a an expiration time, but it deletes the entire document and not just an individual field. To resolve this issue, Redis will be used instead. Redis feels just to be the right tool for the right job. Heroku offers a free tier, it's plenty enough. Typegoose @ Property ({ expires : \"30s\" , default : Date . now () }) changePassword : Date ;","title":"Time To Live MongoDB"},{"location":"5.Database/1.mongodb.html","text":"MongoDB Database Connection Typescript import \"dotenv/config\" ; import mongoose from \"mongoose\" ; export default async () => { let connection ; if ( process . env . NODE_ENV2 === \"production\" ) { connection = process . env . MONGO_PRODUCTION ; } if ( process . env . NODE_ENV2 === \"development\" ) { connection = process . env . MONGO_DEVELOPMENT ; } if ( process . env . NODE_ENV2 === \"test\" ) { connection = process . env . MONGO_TEST ; } try { await mongoose . connect ( connection || process . env . MONGO_ATLAS ! , { useNewUrlParser : true , useCreateIndex : true , useUnifiedTopology : true , useFindAndModify : false , }); await console . log ( \"Connected to database\" ); } catch ( error ) { console . log ( error . message ); console . log ( \"Couldn't connect to database\" ); } }; Development Data Existing MongoDB data can be setup quickly by restoring the available backup in the project. Setup MongoDB Data mongorestore -d api_dev_db --host localhost:27017 \\ --username api_user \\ --password api1234 \\ --authenticationDatabase api_dev_db \\ --dir ./api_dev_db \\ --gzip Development, Production & Testing Database When using docker-compose with Make , Docker creates 3 distinct databases for multiple environements, one for development, production and testing. The following Javascript code is injected when Docker starts MongoDB, it create 3 users with their roles and credentials. print ( \"Start #################################################################\" ); // Production db = db . getSiblingDB ( \"api_prod_db\" ); db . createUser ({ user : \"api_user\" , pwd : \"api1234\" , roles : [{ role : \"readWrite\" , db : \"api_prod_db\" }], }); db . createCollection ( \"users\" ); // Developpement db = db . getSiblingDB ( \"api_dev_db\" ); db . createUser ({ user : \"api_user\" , pwd : \"api1234\" , roles : [{ role : \"readWrite\" , db : \"api_dev_db\" }], }); db . createCollection ( \"users\" ); // Test db = db . getSiblingDB ( \"api_test_db\" ); db . createUser ({ user : \"api_user\" , pwd : \"api1234\" , roles : [{ role : \"readWrite\" , db : \"api_test_db\" }], }); db . createCollection ( \"users\" ); print ( \"END #################################################################\" ); Sources Source Kind URI MongoDB University Tutorial Link MongoDB Documentation Documentation Link","title":"MongoDB"},{"location":"5.Database/1.mongodb.html#mongodb","text":"","title":"MongoDB"},{"location":"5.Database/1.mongodb.html#database-connection","text":"Typescript import \"dotenv/config\" ; import mongoose from \"mongoose\" ; export default async () => { let connection ; if ( process . env . NODE_ENV2 === \"production\" ) { connection = process . env . MONGO_PRODUCTION ; } if ( process . env . NODE_ENV2 === \"development\" ) { connection = process . env . MONGO_DEVELOPMENT ; } if ( process . env . NODE_ENV2 === \"test\" ) { connection = process . env . MONGO_TEST ; } try { await mongoose . connect ( connection || process . env . MONGO_ATLAS ! , { useNewUrlParser : true , useCreateIndex : true , useUnifiedTopology : true , useFindAndModify : false , }); await console . log ( \"Connected to database\" ); } catch ( error ) { console . log ( error . message ); console . log ( \"Couldn't connect to database\" ); } };","title":"Database Connection"},{"location":"5.Database/1.mongodb.html#development-data","text":"Existing MongoDB data can be setup quickly by restoring the available backup in the project. Setup MongoDB Data mongorestore -d api_dev_db --host localhost:27017 \\ --username api_user \\ --password api1234 \\ --authenticationDatabase api_dev_db \\ --dir ./api_dev_db \\ --gzip","title":"Development Data"},{"location":"5.Database/1.mongodb.html#development-production-testing-database","text":"When using docker-compose with Make , Docker creates 3 distinct databases for multiple environements, one for development, production and testing. The following Javascript code is injected when Docker starts MongoDB, it create 3 users with their roles and credentials. print ( \"Start #################################################################\" ); // Production db = db . getSiblingDB ( \"api_prod_db\" ); db . createUser ({ user : \"api_user\" , pwd : \"api1234\" , roles : [{ role : \"readWrite\" , db : \"api_prod_db\" }], }); db . createCollection ( \"users\" ); // Developpement db = db . getSiblingDB ( \"api_dev_db\" ); db . createUser ({ user : \"api_user\" , pwd : \"api1234\" , roles : [{ role : \"readWrite\" , db : \"api_dev_db\" }], }); db . createCollection ( \"users\" ); // Test db = db . getSiblingDB ( \"api_test_db\" ); db . createUser ({ user : \"api_user\" , pwd : \"api1234\" , roles : [{ role : \"readWrite\" , db : \"api_test_db\" }], }); db . createCollection ( \"users\" ); print ( \"END #################################################################\" );","title":"Development, Production &amp; Testing Database"},{"location":"5.Database/1.mongodb.html#sources","text":"Source Kind URI MongoDB University Tutorial Link MongoDB Documentation Documentation Link","title":"Sources"},{"location":"5.Database/2.schema.html","text":"Database Schema Moon Modeler Moon Modeler is a data modeling tool for MongoDB, PostgreSQL, MySQL, MariaDB, SQLite and GraphQL. Er diagrams can be drawn manually or they can be reversed engineered by establishing a connection with the database. BlueberryShop Database Schema: Click For Opening It Updating Existing Diagram Updating the database diagram can be done by loading the BlueberryShopDiagram.dmm located in the Mongo folder into Moon Modeler. Mongo Folder \u251c\u2500\u2500 BlueberryShopDiagram.dmm \u251c\u2500\u2500 BlueberryShopDiagram.pdf \u2514\u2500\u2500 mongo-init.js Sources Source URI Moon Modeler Link","title":"Database Schema"},{"location":"5.Database/2.schema.html#database-schema","text":"","title":"Database Schema"},{"location":"5.Database/2.schema.html#moon-modeler","text":"Moon Modeler is a data modeling tool for MongoDB, PostgreSQL, MySQL, MariaDB, SQLite and GraphQL. Er diagrams can be drawn manually or they can be reversed engineered by establishing a connection with the database. BlueberryShop Database Schema: Click For Opening It","title":"Moon Modeler"},{"location":"5.Database/2.schema.html#updating-existing-diagram","text":"Updating the database diagram can be done by loading the BlueberryShopDiagram.dmm located in the Mongo folder into Moon Modeler. Mongo Folder \u251c\u2500\u2500 BlueberryShopDiagram.dmm \u251c\u2500\u2500 BlueberryShopDiagram.pdf \u2514\u2500\u2500 mongo-init.js","title":"Updating Existing Diagram"},{"location":"5.Database/2.schema.html#sources","text":"Source URI Moon Modeler Link","title":"Sources"},{"location":"5.Database/3.horizontal.html","text":"Horizontal Scaling Vertical Scaling Personal Diagram Horizontal Scaling Personal Diagram","title":"Horizontal Scaling"},{"location":"5.Database/3.horizontal.html#horizontal-scaling","text":"Vertical Scaling Personal Diagram Horizontal Scaling Personal Diagram","title":"Horizontal Scaling"},{"location":"5.Database/4.backup.html","text":"Backups MongoDB backup creation can be done with the help of the utility commande line mongodump and mongorestore . In the Script folder, bash scripts are already available. Backup Script mongodump --host localhost:27017 \\ --username api_user \\ --password api1234 \\ --db api_dev_db \\ --gzip \\ --out ../Backup/ ` date + \"%Y-%m-%d\" ` Restoration Script mongorestore -d api_dev_db --host localhost:27017 \\ --username api_user \\ --password api1234 \\ --authenticationDatabase api_dev_db \\ --dir ../Backup/ ` date + \"%Y-%m-%d\" /api_dev_db ` \\ --gzip User credentials in development environment Warning Change your credentials in production. Email Password blueberry@shop.com 123456789 Sources Source Athor Link mongodump MongoDB Documentation Link mongorestore MongoDB Documentation Link","title":"Backups"},{"location":"5.Database/4.backup.html#backups","text":"MongoDB backup creation can be done with the help of the utility commande line mongodump and mongorestore . In the Script folder, bash scripts are already available. Backup Script mongodump --host localhost:27017 \\ --username api_user \\ --password api1234 \\ --db api_dev_db \\ --gzip \\ --out ../Backup/ ` date + \"%Y-%m-%d\" ` Restoration Script mongorestore -d api_dev_db --host localhost:27017 \\ --username api_user \\ --password api1234 \\ --authenticationDatabase api_dev_db \\ --dir ../Backup/ ` date + \"%Y-%m-%d\" /api_dev_db ` \\ --gzip","title":"Backups"},{"location":"5.Database/4.backup.html#user-credentials-in-development-environment","text":"Warning Change your credentials in production. Email Password blueberry@shop.com 123456789","title":"User credentials in development environment"},{"location":"5.Database/4.backup.html#sources","text":"Source Athor Link mongodump MongoDB Documentation Link mongorestore MongoDB Documentation Link","title":"Sources"},{"location":"6.Hosting/1.Production.html","text":"Production Commissioning Guide In this guide, a production installation of BlueberryShop will be showed, the project will be hosted in a virtual machine using Ubuntu Server 20.04.2 LTS as the main OS. Note that system related topics like network security, virtual machines and SSH will not be covered, it is expected that you already have a production environment. For more security information, feel free to check Nginx , Nodejs , MongoDB or Redis 's documentation. Install Git First install Git, then download with it BlueberryShop from Github. Git Installation # Update & upgrade existing packages sudo apt update sudo apt upgrade sudo apt install git Install BlueberryShop git clone https://github.com/besSejrani/BlueberryShop.git Install Node.js Ubuntu cd ~ curl -sL https://deb.nodesource.com/setup_14.x -o nodesource_setup.sh # Verify Shell script nano nodesource_setup.sh # Execute Shell script sudo bash nodesource_setup.sh # Install Node.js sudo apt install nodejs # Verify Node.js version node -v Install Make Make # Install package sudo apt install make Install Redis Install Redis # Redis needs a C compiler and some utility packages sudo apt install gcc \\ pkg-config \\ tcl-dev wget https://download.redis.io/releases/redis-6.2.3.tar.gz tar xzf redis-6.2.3.tar.gz cd redis-6.2.3 sudo make install Testing Redis make test Run Redis Server cd redis-6.2.3 redis-server Install MongoDB # Import public key of the package management system wget -qO - https://www.mongodb.org/static/pgp/server-4.4.asc | sudo apt-key add - # Create the /etc/apt/sources.list.d/mongodb-org-4.4.list file echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/4.4 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.4.list # Reload Packages sudo apt-get update # Install MongoDB sudo apt-get install -y mongodb-org Start Mongod sudo systemctl start mongod Stop Mongod sudo systemctl stop mongod Reload Mongod Daemon sudo systemctl daemon-reload Mongod Status sudo systemctl status mongod Enable Mongod sudo systemctl enable mongod # Run MongoDB mongo Install Nginx Install From Source Code Install Nginx # Update & upgrade existing packages sudo apt update sudo apt upgrade # Download source from nginx.org wget https://nginx.org/download/nginx-1.20.1.tar.gz tar -zxvf nginx-1.20.1.tar.gz # Rename folder mv nginx-1.20.1 nginx # Install Nginx Modules sudo apt install libpcre3 \\ libpcre3-dev \\ zlib1g \\ zlib1g-dev \\ libssl-dev \\ libgd-dev cd nginx ./configure --sbin-path = /usr/bin/nginx \\ --conf-path = /etc/nginx/nginx.conf \\ --error-log-path = /var/log/nginx/error.log \\ --http-log-path = /var/log/nginx/access.log \\ --with-pcre \\ --pid-path = /var/run/nginx.pid \\ --with-http_ssl_module \\ --with-http_image_filter_module \\ --modules-path = /etc/nginx/modules \\ --with-http_v2_module sudo make install Custom Systemctl Script Systemctl Script sudo vim /lib/systemd/system/nginx.service [Unit] Description=The NGINX HTTP and reverse proxy server After=syslog.target network-online.target remote-fs.target nss-lookup.target Wants=network-online.target [Service] Type=forking PIDFile=/run/nginx.pid ExecStartPre=/usr/bin/nginx -t ExecStart=/usr/bin/nginx ExecReload=/usr/bin/nginx -s reload ExecStop=/bin/kill -s QUIT $MAINPID PrivateTmp=true [Install] WantedBy=multi-user.target Start Nginx systemctl start nginx Enable Nginx systemctl enable nginx Disable Nginx systemctl disable start Restart Nginx systemctl restart nginx Nginx Status systemctl status nginx Nginx Configuration File If you have decided to change the Client or Server port to something different than 3000 and 4000, than modify the upstream client and the upstream server, otherwise Nginx can not reverse proxy incoming requests. If you use a TLS certificate, like you should, change ssl_certificate and ssl_certificate_key parameters with your TLS certificate. Nginx default.conf File worker_processes auto ; events { worker_connections 1024 ; } http { include /etc/nginx/conf.d/*.conf ; upstream client { server 127 .0.0.1:3000 ; } upstream server { server 127 .0.0.1:4000 ; } #Virtual server host #Redirect all trafic to HTTPS server { listen 80 ; server_name localhost ; return 301 https:// $host$request_uri ; } server { #Port Nginx should listen to and HTTP version #No SSL is used, actually its TLS listen 443 ssl http2 ; #Define server name, ip address, localhost, domain name, ... server_name localhost ; #Disable SSL ssl_protocols TLSv1 TLSv1.1 TLSv1.2 ; #Optimise cipher suits ssl_prefer_server_ciphers on ; ssl_ciphers ECDH+AESGCM:ECDH+AES256:ECDH+AES128:DH+3DES:!ADH:!AECDH:!MD5 ; #Enable DH(Diffie-Hellman) params, alow to do key exchange with perfect secrety #ssl_dhparam /etc/nginx/ssl/dhparam.pem; #Enable HSTS (HTTP Strict Transport Security) #Header telling to load nothing over HTTP add_header Strict-Transport-Security 'max-age=31536000' always ; #TLS sessions, cache TLS handshake ssl_session_cache shared:SSL:40m ; ssl_session_timeout 4h ; #Specify path to SSL/TLS certificate and public key ssl_certificate /etc/nginx/server.crt ; ssl_certificate_key /etc/nginx/server.key ; #Avoid X-Frame, can leed to clickjacking attack add_header X-Frame-Options \"SAMEORIGIN\" ; #Mitigate XSS attacks add_header X-XSS-Protection \"1; mode=block\" ; # add_header X-Content-Type-Options \"nosniff\" ; location / { proxy_pass http://client ; } location /sockjs-node { proxy_pass http://client ; proxy_http_version 1 .1 ; proxy_set_header Upgrade $http_upgrade ; proxy_set_header Connection \"Upgrade\" ; } location /server { rewrite /api/ ( .* ) / $1 break ; proxy_pass http://server ; } } } Generate TLS Certificate Script sudo vim generate.sh #!bin/bash openssl req \\ -newkey rsa:2048 \\ -x509 \\ -nodes \\ -keyout server.key \\ -new \\ -out server.crt \\ -config ./openssl-custom.cnf \\ -sha256 \\ -days 7300 TLS Information sudo vim openssl-custom.cnf [ req ] default_bits = 2048 prompt = no default_md = sha256 x509_extensions = v3_req distinguished_name = dn [ dn ] C = US ST = KS L = Olathe O = IT OU = IT Department emailAddress = webmaster@example.com CN = localhost [ v3_req ] subjectAltName = @alt_names [ alt_names ] DNS.1 = *.localhost DNS.2 = localhost # copy script to nginx folder cp generate.sh /etc/nginx # TLS information file cp openssl-custom.cnf /etc/nginx # Execute script, it generates TLS certificate sudo bash ./generate.sh Application Configuration Install Project Dependencies Client // NPM cd Client npm install // Yarn cd Client yarn install Server // NPM cd Server npm install // Yarn cd Server yarn install Create & Configure Server Environment Variables All environment variables needed for the server can be found in the Server section, under Configuration . .env # Install VIM text editor, you can also use nano sudo apt install vim # After the .env file is created, add and modify the content cd /BlueberryShop/Server vim .env Create & Configure Client Environment Variables All environment variables needed for the client can be found in the Client section, under Configuration . .env # Install VIM text editor, you can also use nano sudo apt install vim # After the .env file is created, add and modify the content cd /BlueberryShop/Client vim .env Application Configuration Apollo Client Make sure that the uri variable corresponds to your Server URI. You can change your NEXT_PUBLIC_DOCKER or NEXT_PUBLIC_DEVELOPMENT_SERVER environment variable value by modifying the environment variable file in Client . cd ~/BlueberryShop/Client/Apollo vim ssr.tsx let uri ; if ( typeof window ! == \"undefined\" && process.env.NODE_ENV == \"development\" ) { uri = process.env.NEXT_PUBLIC_DEVELOPMENT_SERVER ; } else { uri = process.env.NEXT_PUBLIC_DOCKER || process.env.NEXT_PUBLIC_DEVELOPMENT_SERVER ; } CORS Make sure that the CORS settings corresponds to your Client URI, otherwise, the client can't make any request to the server. You can change your CORS_DOMAIN environment variable value by modifying the environment variable file in Server . cd ~/BlueberryShop/Server/src/Express vim index.ts # // CORS Configuration // CORS Configuration const corsOptions = { origin: process.env.CORS_DOMAIN, credentials: true, } ; Cookies Make sure that the Cookies settings corresponds to your domain, otherwise, the client will not receive any cookie when connecting to the application. Cookies are used for authentication. You can change your COOKIES_DOMAIN environment variable value by modifying the environment variable file in Server . cd ~/BlueberryShop/Server/src/Graphql/resolvers/authentication vim Signin.ts context?.res?.cookie ( \"token\" , token, { maxAge: ( 60 * 60 * 1000 * 24 * 1 ) as number, httpOnly: true, domain: process.env.COOKIES_DOMAIN, path: \"/\" , secure: true, sameSite: \"none\" , }) ; Run Application Script // NPM cd Server npm run dev // Yarn cd Server yarn run dev","title":"Production"},{"location":"6.Hosting/1.Production.html#production","text":"","title":"Production"},{"location":"6.Hosting/1.Production.html#commissioning-guide","text":"In this guide, a production installation of BlueberryShop will be showed, the project will be hosted in a virtual machine using Ubuntu Server 20.04.2 LTS as the main OS. Note that system related topics like network security, virtual machines and SSH will not be covered, it is expected that you already have a production environment. For more security information, feel free to check Nginx , Nodejs , MongoDB or Redis 's documentation.","title":"Commissioning Guide"},{"location":"6.Hosting/1.Production.html#install-git","text":"First install Git, then download with it BlueberryShop from Github. Git Installation # Update & upgrade existing packages sudo apt update sudo apt upgrade sudo apt install git Install BlueberryShop git clone https://github.com/besSejrani/BlueberryShop.git","title":"Install Git"},{"location":"6.Hosting/1.Production.html#install-nodejs","text":"Ubuntu cd ~ curl -sL https://deb.nodesource.com/setup_14.x -o nodesource_setup.sh # Verify Shell script nano nodesource_setup.sh # Execute Shell script sudo bash nodesource_setup.sh # Install Node.js sudo apt install nodejs # Verify Node.js version node -v","title":"Install Node.js"},{"location":"6.Hosting/1.Production.html#install-make","text":"Make # Install package sudo apt install make","title":"Install Make"},{"location":"6.Hosting/1.Production.html#install-redis","text":"Install Redis # Redis needs a C compiler and some utility packages sudo apt install gcc \\ pkg-config \\ tcl-dev wget https://download.redis.io/releases/redis-6.2.3.tar.gz tar xzf redis-6.2.3.tar.gz cd redis-6.2.3 sudo make install Testing Redis make test Run Redis Server cd redis-6.2.3 redis-server","title":"Install Redis"},{"location":"6.Hosting/1.Production.html#install-mongodb","text":"# Import public key of the package management system wget -qO - https://www.mongodb.org/static/pgp/server-4.4.asc | sudo apt-key add - # Create the /etc/apt/sources.list.d/mongodb-org-4.4.list file echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/4.4 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.4.list # Reload Packages sudo apt-get update # Install MongoDB sudo apt-get install -y mongodb-org Start Mongod sudo systemctl start mongod Stop Mongod sudo systemctl stop mongod Reload Mongod Daemon sudo systemctl daemon-reload Mongod Status sudo systemctl status mongod Enable Mongod sudo systemctl enable mongod # Run MongoDB mongo","title":"Install MongoDB"},{"location":"6.Hosting/1.Production.html#install-nginx","text":"","title":"Install Nginx"},{"location":"6.Hosting/1.Production.html#install-from-source-code","text":"Install Nginx # Update & upgrade existing packages sudo apt update sudo apt upgrade # Download source from nginx.org wget https://nginx.org/download/nginx-1.20.1.tar.gz tar -zxvf nginx-1.20.1.tar.gz # Rename folder mv nginx-1.20.1 nginx # Install Nginx Modules sudo apt install libpcre3 \\ libpcre3-dev \\ zlib1g \\ zlib1g-dev \\ libssl-dev \\ libgd-dev cd nginx ./configure --sbin-path = /usr/bin/nginx \\ --conf-path = /etc/nginx/nginx.conf \\ --error-log-path = /var/log/nginx/error.log \\ --http-log-path = /var/log/nginx/access.log \\ --with-pcre \\ --pid-path = /var/run/nginx.pid \\ --with-http_ssl_module \\ --with-http_image_filter_module \\ --modules-path = /etc/nginx/modules \\ --with-http_v2_module sudo make install","title":"Install From Source Code"},{"location":"6.Hosting/1.Production.html#custom-systemctl-script","text":"Systemctl Script sudo vim /lib/systemd/system/nginx.service [Unit] Description=The NGINX HTTP and reverse proxy server After=syslog.target network-online.target remote-fs.target nss-lookup.target Wants=network-online.target [Service] Type=forking PIDFile=/run/nginx.pid ExecStartPre=/usr/bin/nginx -t ExecStart=/usr/bin/nginx ExecReload=/usr/bin/nginx -s reload ExecStop=/bin/kill -s QUIT $MAINPID PrivateTmp=true [Install] WantedBy=multi-user.target Start Nginx systemctl start nginx Enable Nginx systemctl enable nginx Disable Nginx systemctl disable start Restart Nginx systemctl restart nginx Nginx Status systemctl status nginx","title":"Custom Systemctl Script"},{"location":"6.Hosting/1.Production.html#nginx-configuration-file","text":"If you have decided to change the Client or Server port to something different than 3000 and 4000, than modify the upstream client and the upstream server, otherwise Nginx can not reverse proxy incoming requests. If you use a TLS certificate, like you should, change ssl_certificate and ssl_certificate_key parameters with your TLS certificate. Nginx default.conf File worker_processes auto ; events { worker_connections 1024 ; } http { include /etc/nginx/conf.d/*.conf ; upstream client { server 127 .0.0.1:3000 ; } upstream server { server 127 .0.0.1:4000 ; } #Virtual server host #Redirect all trafic to HTTPS server { listen 80 ; server_name localhost ; return 301 https:// $host$request_uri ; } server { #Port Nginx should listen to and HTTP version #No SSL is used, actually its TLS listen 443 ssl http2 ; #Define server name, ip address, localhost, domain name, ... server_name localhost ; #Disable SSL ssl_protocols TLSv1 TLSv1.1 TLSv1.2 ; #Optimise cipher suits ssl_prefer_server_ciphers on ; ssl_ciphers ECDH+AESGCM:ECDH+AES256:ECDH+AES128:DH+3DES:!ADH:!AECDH:!MD5 ; #Enable DH(Diffie-Hellman) params, alow to do key exchange with perfect secrety #ssl_dhparam /etc/nginx/ssl/dhparam.pem; #Enable HSTS (HTTP Strict Transport Security) #Header telling to load nothing over HTTP add_header Strict-Transport-Security 'max-age=31536000' always ; #TLS sessions, cache TLS handshake ssl_session_cache shared:SSL:40m ; ssl_session_timeout 4h ; #Specify path to SSL/TLS certificate and public key ssl_certificate /etc/nginx/server.crt ; ssl_certificate_key /etc/nginx/server.key ; #Avoid X-Frame, can leed to clickjacking attack add_header X-Frame-Options \"SAMEORIGIN\" ; #Mitigate XSS attacks add_header X-XSS-Protection \"1; mode=block\" ; # add_header X-Content-Type-Options \"nosniff\" ; location / { proxy_pass http://client ; } location /sockjs-node { proxy_pass http://client ; proxy_http_version 1 .1 ; proxy_set_header Upgrade $http_upgrade ; proxy_set_header Connection \"Upgrade\" ; } location /server { rewrite /api/ ( .* ) / $1 break ; proxy_pass http://server ; } } } Generate TLS Certificate Script sudo vim generate.sh #!bin/bash openssl req \\ -newkey rsa:2048 \\ -x509 \\ -nodes \\ -keyout server.key \\ -new \\ -out server.crt \\ -config ./openssl-custom.cnf \\ -sha256 \\ -days 7300 TLS Information sudo vim openssl-custom.cnf [ req ] default_bits = 2048 prompt = no default_md = sha256 x509_extensions = v3_req distinguished_name = dn [ dn ] C = US ST = KS L = Olathe O = IT OU = IT Department emailAddress = webmaster@example.com CN = localhost [ v3_req ] subjectAltName = @alt_names [ alt_names ] DNS.1 = *.localhost DNS.2 = localhost # copy script to nginx folder cp generate.sh /etc/nginx # TLS information file cp openssl-custom.cnf /etc/nginx # Execute script, it generates TLS certificate sudo bash ./generate.sh","title":"Nginx Configuration File"},{"location":"6.Hosting/1.Production.html#application-configuration","text":"","title":"Application Configuration"},{"location":"6.Hosting/1.Production.html#install-project-dependencies","text":"Client // NPM cd Client npm install // Yarn cd Client yarn install Server // NPM cd Server npm install // Yarn cd Server yarn install","title":"Install Project Dependencies"},{"location":"6.Hosting/1.Production.html#create-configure-server-environment-variables","text":"All environment variables needed for the server can be found in the Server section, under Configuration . .env # Install VIM text editor, you can also use nano sudo apt install vim # After the .env file is created, add and modify the content cd /BlueberryShop/Server vim .env","title":"Create &amp; Configure Server Environment Variables"},{"location":"6.Hosting/1.Production.html#create-configure-client-environment-variables","text":"All environment variables needed for the client can be found in the Client section, under Configuration . .env # Install VIM text editor, you can also use nano sudo apt install vim # After the .env file is created, add and modify the content cd /BlueberryShop/Client vim .env","title":"Create &amp; Configure Client Environment Variables"},{"location":"6.Hosting/1.Production.html#application-configuration_1","text":"Apollo Client Make sure that the uri variable corresponds to your Server URI. You can change your NEXT_PUBLIC_DOCKER or NEXT_PUBLIC_DEVELOPMENT_SERVER environment variable value by modifying the environment variable file in Client . cd ~/BlueberryShop/Client/Apollo vim ssr.tsx let uri ; if ( typeof window ! == \"undefined\" && process.env.NODE_ENV == \"development\" ) { uri = process.env.NEXT_PUBLIC_DEVELOPMENT_SERVER ; } else { uri = process.env.NEXT_PUBLIC_DOCKER || process.env.NEXT_PUBLIC_DEVELOPMENT_SERVER ; } CORS Make sure that the CORS settings corresponds to your Client URI, otherwise, the client can't make any request to the server. You can change your CORS_DOMAIN environment variable value by modifying the environment variable file in Server . cd ~/BlueberryShop/Server/src/Express vim index.ts # // CORS Configuration // CORS Configuration const corsOptions = { origin: process.env.CORS_DOMAIN, credentials: true, } ; Cookies Make sure that the Cookies settings corresponds to your domain, otherwise, the client will not receive any cookie when connecting to the application. Cookies are used for authentication. You can change your COOKIES_DOMAIN environment variable value by modifying the environment variable file in Server . cd ~/BlueberryShop/Server/src/Graphql/resolvers/authentication vim Signin.ts context?.res?.cookie ( \"token\" , token, { maxAge: ( 60 * 60 * 1000 * 24 * 1 ) as number, httpOnly: true, domain: process.env.COOKIES_DOMAIN, path: \"/\" , secure: true, sameSite: \"none\" , }) ;","title":"Application Configuration"},{"location":"6.Hosting/1.Production.html#run-application","text":"Script // NPM cd Server npm run dev // Yarn cd Server yarn run dev","title":"Run Application"},{"location":"7.Security/0.headers.html","text":"Security Headers Helmet Helmet is a Node.js middleware which allows to set security headers. By default, it adds a Powered-By header, adds a XSS header and much more. Like mentioned on their website, it's not a silver bullet, but it can help. For the moment, the content security policy is set to false, since it blocks the GraphQL Playground, it is temporary. Typescript import helmet from \"helmet\" ; app . use ( helmet ({ contentSecurityPolicy : false })); Libraries Library URI Description helmet Link","title":"Security Headers"},{"location":"7.Security/0.headers.html#security-headers","text":"","title":"Security Headers"},{"location":"7.Security/0.headers.html#helmet","text":"Helmet is a Node.js middleware which allows to set security headers. By default, it adds a Powered-By header, adds a XSS header and much more. Like mentioned on their website, it's not a silver bullet, but it can help. For the moment, the content security policy is set to false, since it blocks the GraphQL Playground, it is temporary. Typescript import helmet from \"helmet\" ; app . use ( helmet ({ contentSecurityPolicy : false }));","title":"Helmet"},{"location":"7.Security/0.headers.html#libraries","text":"Library URI Description helmet Link","title":"Libraries"},{"location":"7.Security/1.sanitization.html","text":"Data Sanitization Frontend Sanitization Validating user inputs is very important since they can enter what ever they want and perform for example XSS, SQL injections and CSRF attacks. Modern frontend Javascript frameworks like React, Vue and Angular have already XSS validation in place. It doesn't mean that the frontend inputs are validated that the server shouldn't also validate incoming data. There is a lot of ways for sending data to the server without the need of a browser, like CURL, Postman API tool and many more. Typescript Input Validation import React from \"react\" ; // React-Hook-Form import { ErrorMessage } from \"@hookform/error-message\" ; // Material-UI import { TextField , Typography } from \"@material-ui/core\" ; // =========================================================== type inputType = { type : string ; name : string ; id : string ; label : string ; multiline? : boolean ; rowsMax? : string ; inputRef : any ; value : string | number ; onChange : Function ; errors : any ; }; const inputForm : React.FC < inputType > = ({ type , name , id , label , multiline , rowsMax , inputRef , value , onChange , errors , }) => { return ( <> < TextField style = {{ margin : \"5px\" }} type = { type } name = { name } id = { id } label = { label } multiline = { multiline } rowsMax = { rowsMax } inputRef = { inputRef } value = { value } onChange = {( text ) => onChange ( text . target . value )} /> < ErrorMessage errors = { errors } name = { name } as = { < Typography variant = \"body2\" /> } > {({ messages }) => messages && Object . entries ( messages ). map (([ type , message ]) => ( < p key = { type } > { message } < /p> )) } < /ErrorMessage> < /> ); }; export default inputForm ; Server Sanitization For validating GraphQL input requests, make sur to first set validate to true in your GraphQL schema. With the help of class-validator , a validation library, GraphQL inputs can be validated by decorating the inputs with validation properties GraphQL Schema // build TypeGraphQL executable schema export default async function createSchema () : Promise < GraphQLSchema > { const schema = await buildSchema ({ // 1. add all typescript resolvers resolvers : [ ` ${ __dirname } /../resolvers/**/*.ts` ], emitSchemaFile : path.resolve ( __dirname , \"schema.gql\" ), // 2. use document converting middleware globalMiddlewares : [ TypegooseMiddleware ], // 3. use ObjectId scalar mapping scalarsMap : [{ type : ObjectId , scalar : ObjectIdScalar }], validate : true , }); return schema ; GraphQL Input Validation Example // Class-Validator import { MaxLength } from \"class-validator\" ; // GraphQL import { InputType , Field } from \"type-graphql\" ; // ======================================================================================================== @InputType () export class CreateReviewInput { @Field () rating : string ; @Field () @MaxLength ( 250 , { message : \"Product name can not be longer than 250 characters\" }) review : string ; } Libraries Library URI Description class-validator Link mongoose Link express-mongo-sanitize Link","title":"Data Sanitization"},{"location":"7.Security/1.sanitization.html#data-sanitization","text":"","title":"Data Sanitization"},{"location":"7.Security/1.sanitization.html#frontend-sanitization","text":"Validating user inputs is very important since they can enter what ever they want and perform for example XSS, SQL injections and CSRF attacks. Modern frontend Javascript frameworks like React, Vue and Angular have already XSS validation in place. It doesn't mean that the frontend inputs are validated that the server shouldn't also validate incoming data. There is a lot of ways for sending data to the server without the need of a browser, like CURL, Postman API tool and many more. Typescript Input Validation import React from \"react\" ; // React-Hook-Form import { ErrorMessage } from \"@hookform/error-message\" ; // Material-UI import { TextField , Typography } from \"@material-ui/core\" ; // =========================================================== type inputType = { type : string ; name : string ; id : string ; label : string ; multiline? : boolean ; rowsMax? : string ; inputRef : any ; value : string | number ; onChange : Function ; errors : any ; }; const inputForm : React.FC < inputType > = ({ type , name , id , label , multiline , rowsMax , inputRef , value , onChange , errors , }) => { return ( <> < TextField style = {{ margin : \"5px\" }} type = { type } name = { name } id = { id } label = { label } multiline = { multiline } rowsMax = { rowsMax } inputRef = { inputRef } value = { value } onChange = {( text ) => onChange ( text . target . value )} /> < ErrorMessage errors = { errors } name = { name } as = { < Typography variant = \"body2\" /> } > {({ messages }) => messages && Object . entries ( messages ). map (([ type , message ]) => ( < p key = { type } > { message } < /p> )) } < /ErrorMessage> < /> ); }; export default inputForm ;","title":"Frontend Sanitization"},{"location":"7.Security/1.sanitization.html#server-sanitization","text":"For validating GraphQL input requests, make sur to first set validate to true in your GraphQL schema. With the help of class-validator , a validation library, GraphQL inputs can be validated by decorating the inputs with validation properties GraphQL Schema // build TypeGraphQL executable schema export default async function createSchema () : Promise < GraphQLSchema > { const schema = await buildSchema ({ // 1. add all typescript resolvers resolvers : [ ` ${ __dirname } /../resolvers/**/*.ts` ], emitSchemaFile : path.resolve ( __dirname , \"schema.gql\" ), // 2. use document converting middleware globalMiddlewares : [ TypegooseMiddleware ], // 3. use ObjectId scalar mapping scalarsMap : [{ type : ObjectId , scalar : ObjectIdScalar }], validate : true , }); return schema ; GraphQL Input Validation Example // Class-Validator import { MaxLength } from \"class-validator\" ; // GraphQL import { InputType , Field } from \"type-graphql\" ; // ======================================================================================================== @InputType () export class CreateReviewInput { @Field () rating : string ; @Field () @MaxLength ( 250 , { message : \"Product name can not be longer than 250 characters\" }) review : string ; }","title":"Server Sanitization"},{"location":"7.Security/1.sanitization.html#libraries","text":"Library URI Description class-validator Link mongoose Link express-mongo-sanitize Link","title":"Libraries"},{"location":"7.Security/2.injection.html","text":"Injections Express-mongo-sanitize Like SQL databases, injections can be performed by malicious users against a NoSQL database. Express-mongo-sanitize is a middleware, it will sanitize the incoming requests and replace the injections with ASCII characters. Typescript import mongoSanitize from \"express-mongo-sanitize\" ; app . use ( mongoSanitize ());","title":"Injections"},{"location":"7.Security/2.injection.html#injections","text":"","title":"Injections"},{"location":"7.Security/2.injection.html#express-mongo-sanitize","text":"Like SQL databases, injections can be performed by malicious users against a NoSQL database. Express-mongo-sanitize is a middleware, it will sanitize the incoming requests and replace the injections with ASCII characters. Typescript import mongoSanitize from \"express-mongo-sanitize\" ; app . use ( mongoSanitize ());","title":"Express-mongo-sanitize"},{"location":"7.Security/3.hashing.html","text":"Hashing Hashing is a unique process of transforming data of variable length into fixed length. This process can also be composed of a secret and a salt in order to limit mass dictionary attacks. Algorithm Length MD5 128 bits, not recommended SHA-1 160 bits SHA-256 256 bits SHA-512 512 bits Bcrypt This hash function is very slow in order to dissuade malicious people from making mass attacks. Scrypt This hash function is very expensive in RAM, making the bulk attack very expensive. Bcryptjs Bcryptjs is a NPM package which allows to implement the Bcrypt algorithm, it's simple API is easy to use. Hash Data import bcrypt from \"bcryptjs\" const salt = await bcrypt . genSalt ( 12 ); const hash = await bcrypt . hash ( password , salt ); Compare Hash import bcrypt from \"bcryptjs\" ; const valid = await bcrypt . compare ( password , user . password ); Libraries Library URI Description bcryptjs Link","title":"Hashing"},{"location":"7.Security/3.hashing.html#hashing","text":"Hashing is a unique process of transforming data of variable length into fixed length. This process can also be composed of a secret and a salt in order to limit mass dictionary attacks. Algorithm Length MD5 128 bits, not recommended SHA-1 160 bits SHA-256 256 bits SHA-512 512 bits Bcrypt This hash function is very slow in order to dissuade malicious people from making mass attacks. Scrypt This hash function is very expensive in RAM, making the bulk attack very expensive.","title":"Hashing"},{"location":"7.Security/3.hashing.html#bcryptjs","text":"Bcryptjs is a NPM package which allows to implement the Bcrypt algorithm, it's simple API is easy to use. Hash Data import bcrypt from \"bcryptjs\" const salt = await bcrypt . genSalt ( 12 ); const hash = await bcrypt . hash ( password , salt ); Compare Hash import bcrypt from \"bcryptjs\" ; const valid = await bcrypt . compare ( password , user . password );","title":"Bcryptjs"},{"location":"7.Security/3.hashing.html#libraries","text":"Library URI Description bcryptjs Link","title":"Libraries"},{"location":"7.Security/4.ratelimiting.html","text":"Rate Limiting Rate limiting allows to limit the number of requests that a user can make on a specific endpoint / resource. Often, the register page is targeted and a brute force attack is performed for finding end user credentials. Limiting the number of requests that a user shouldn't exceed, limits the scope of attack and avoids unnecessary workload on server. The following code comes from one of Ben Awad's Gists Typescript // GraphQL import { MiddlewareFn } from \"type-graphql\" ; import { MyContext } from \"../Graphql/types/MyContext\" ; // In-Memory Database import { redis } from \"../Redis/index\" ; // ============================================================== const oneDay = 60 * 60 * 24 ; export const rateLimit : ( limit? : number ) => MiddlewareFn < MyContext > = ( limit = 50 ) => async ( { context : { req }, info }, next ) => { const key = `rate-limit: ${ info . fieldName } : ${ req . ip } ` ; const current = await redis . incr ( key ); if ( current > limit ) { throw new Error ( \"You are making too much requests\" ); } else if ( current === 1 ) { await redis . expire ( key , oneDay ); } return next (); }; Sources Source Kind URI Rate Limiting Ben Awad Link","title":"Rate Limiting"},{"location":"7.Security/4.ratelimiting.html#rate-limiting","text":"Rate limiting allows to limit the number of requests that a user can make on a specific endpoint / resource. Often, the register page is targeted and a brute force attack is performed for finding end user credentials. Limiting the number of requests that a user shouldn't exceed, limits the scope of attack and avoids unnecessary workload on server. The following code comes from one of Ben Awad's Gists Typescript // GraphQL import { MiddlewareFn } from \"type-graphql\" ; import { MyContext } from \"../Graphql/types/MyContext\" ; // In-Memory Database import { redis } from \"../Redis/index\" ; // ============================================================== const oneDay = 60 * 60 * 24 ; export const rateLimit : ( limit? : number ) => MiddlewareFn < MyContext > = ( limit = 50 ) => async ( { context : { req }, info }, next ) => { const key = `rate-limit: ${ info . fieldName } : ${ req . ip } ` ; const current = await redis . incr ( key ); if ( current > limit ) { throw new Error ( \"You are making too much requests\" ); } else if ( current === 1 ) { await redis . expire ( key , oneDay ); } return next (); };","title":"Rate Limiting"},{"location":"7.Security/4.ratelimiting.html#sources","text":"Source Kind URI Rate Limiting Ben Awad Link","title":"Sources"},{"location":"7.Security/5.cookie.html","text":"Cookie Securing Cookies When a user signs in, a secured cookie is stored on his browser, that cookie is secured against XSS and CSRF attacks context . res . cookie ( \"token\" , token , { maxAge : ( 60 * 60 * 1000 * 24 * 1 ) as number , httpOnly : true , domain : \"localhost\" , path : \"/\" , secure : true , sameSite : \"none\" , });","title":"Cookie"},{"location":"7.Security/5.cookie.html#cookie","text":"","title":"Cookie"},{"location":"7.Security/5.cookie.html#securing-cookies","text":"When a user signs in, a secured cookie is stored on his browser, that cookie is secured against XSS and CSRF attacks context . res . cookie ( \"token\" , token , { maxAge : ( 60 * 60 * 1000 * 24 * 1 ) as number , httpOnly : true , domain : \"localhost\" , path : \"/\" , secure : true , sameSite : \"none\" , });","title":"Securing Cookies"},{"location":"7.Security/6.tls.html","text":"TLS Certificate Information When using docker-compose, Nginx is available as reverse proxy on https://localhost:8443, it uses a self signed certificate. Keep in mind that self signed certificates should only be used in development environment, they must not be used in production ! Let's Encrypt & Certbot A nonprofit Certificate Authority providing TLS certificates to 260 million websites. With the help of Let's Encrypt, there is no excuse for sending plain data over HTTP when HTTPS can easily be implemented in production. Certbot, another great tool, helps to implement Let's Encrypt Certificate on Nginx, Apache, Envoy, ... Although the TLS certificates of Let's Encrypt are easy to implement with the help of Certbot, they last only for 90 days. Use cron jobs to automate scheduling tasks. Sources Source Link Let's Encrypt Link Certbot Link","title":"TLS Certificate"},{"location":"7.Security/6.tls.html#tls-certificate","text":"Information When using docker-compose, Nginx is available as reverse proxy on https://localhost:8443, it uses a self signed certificate. Keep in mind that self signed certificates should only be used in development environment, they must not be used in production !","title":"TLS Certificate"},{"location":"7.Security/6.tls.html#lets-encrypt-certbot","text":"A nonprofit Certificate Authority providing TLS certificates to 260 million websites. With the help of Let's Encrypt, there is no excuse for sending plain data over HTTP when HTTPS can easily be implemented in production. Certbot, another great tool, helps to implement Let's Encrypt Certificate on Nginx, Apache, Envoy, ... Although the TLS certificates of Let's Encrypt are easy to implement with the help of Certbot, they last only for 90 days. Use cron jobs to automate scheduling tasks.","title":"Let's Encrypt &amp; Certbot"},{"location":"7.Security/6.tls.html#sources","text":"Source Link Let's Encrypt Link Certbot Link","title":"Sources"},{"location":"7.Security/7.docker.html","text":"Docker Image Limitation Docker highly recommends to only use trusted images. As for their best practices, it is also recommended to scan docker images in development environment and in production environment. Image: Official Images From DockerHub Image: From Docker Documentation Privileged containers and capabilities By default, docker container does not have many capabilities assigned to them and they are not allowed to access any devices. However, there are several types of capabilities that Linux provides, which gives granular access to containers. If those capabilities are not used, disable them in your Dockerfile or in your orchestration yaml file, just use what you need. Resource Limitation By default, a container can user as much resource as the the host\u2019s kernel scheduler allows. When only using docker by itself, without any orchestration engine like Docker Swarm or Kubernetes, it is recommended to specify resource constraints. CPU Option Description --cpus Specify how much of the available CPU resources a container can use --cpu-period Specify the CPU CFS scheduler period, which is used alongside --cpu-quota. --cpu-quota Impose a CPU CFS quota on the container. The number of microseconds per --cpu-period that the container is limited to before throttled --cpuset-cpus Limit the specific CPUs or cores a container can use. --cpu-shares Set this flag to a value greater or less than the default of 1024 to increase or reduce the container\u2019s weight, and give it access to a greater or lesser proportion of the host machine\u2019s CPU cycles Memory Option Description -m or --memory The maximum amount of memory the container can use. --memory-swap* The amount of memory this container is allowed to swap to disk --memory-swappiness By default, the host kernel can swap out a percentage of anonymous pages used by a container. --memory-reservation Allows you to specify a soft limit smaller than --memory which is activated when Docker detects contention or low memory on the host machine --kernel-memory The maximum amount of kernel memory the container can use. --oom-kill-disable By default, if an out-of-memory (OOM) error occurs, the kernel kills processes in a container. To change this behavior, use the --oom-kill-disable option. Sources Source Link Docker Security Documentation Link Runtime options with Memory, CPUs, and GPUs Link","title":"Docker"},{"location":"7.Security/7.docker.html#docker","text":"","title":"Docker"},{"location":"7.Security/7.docker.html#image-limitation","text":"Docker highly recommends to only use trusted images. As for their best practices, it is also recommended to scan docker images in development environment and in production environment. Image: Official Images From DockerHub Image: From Docker Documentation","title":"Image Limitation"},{"location":"7.Security/7.docker.html#privileged-containers-and-capabilities","text":"By default, docker container does not have many capabilities assigned to them and they are not allowed to access any devices. However, there are several types of capabilities that Linux provides, which gives granular access to containers. If those capabilities are not used, disable them in your Dockerfile or in your orchestration yaml file, just use what you need.","title":"Privileged containers and capabilities"},{"location":"7.Security/7.docker.html#resource-limitation","text":"By default, a container can user as much resource as the the host\u2019s kernel scheduler allows. When only using docker by itself, without any orchestration engine like Docker Swarm or Kubernetes, it is recommended to specify resource constraints. CPU Option Description --cpus Specify how much of the available CPU resources a container can use --cpu-period Specify the CPU CFS scheduler period, which is used alongside --cpu-quota. --cpu-quota Impose a CPU CFS quota on the container. The number of microseconds per --cpu-period that the container is limited to before throttled --cpuset-cpus Limit the specific CPUs or cores a container can use. --cpu-shares Set this flag to a value greater or less than the default of 1024 to increase or reduce the container\u2019s weight, and give it access to a greater or lesser proportion of the host machine\u2019s CPU cycles Memory Option Description -m or --memory The maximum amount of memory the container can use. --memory-swap* The amount of memory this container is allowed to swap to disk --memory-swappiness By default, the host kernel can swap out a percentage of anonymous pages used by a container. --memory-reservation Allows you to specify a soft limit smaller than --memory which is activated when Docker detects contention or low memory on the host machine --kernel-memory The maximum amount of kernel memory the container can use. --oom-kill-disable By default, if an out-of-memory (OOM) error occurs, the kernel kills processes in a container. To change this behavior, use the --oom-kill-disable option.","title":"Resource Limitation"},{"location":"7.Security/7.docker.html#sources","text":"Source Link Docker Security Documentation Link Runtime options with Memory, CPUs, and GPUs Link","title":"Sources"},{"location":"7.Security/8.OWASP.html","text":"OWASP Top Ten Use Case Security shouldn't be an aftertough, like good habits, security best practices and recommendations should be followed. Recommendations Information The following descriptions are coming from the official OWASP Top Ten recommendations. Recommendation Description Injections Injection flaws, such as SQL, NoSQL, OS, and LDAP injection, occur when untrusted data is sent to an interpreter as part of a command or query. The attacker's hostile data can trick the interpreter into executing unintended commands or accessing data without proper authorization. Broken Authentication Application functions related to authentication and session management are often implemented incorrectly, allowing attackers to compromise passwords, keys, or session tokens, or to exploit other implementation flaws to assume other users' identities temporarily or permanently. Sensitive Data Exposure Many web applications and APIs do not properly protect sensitive data, such as financial, healthcare, and PII. Attackers may steal or modify such weakly protected data to conduct credit card fraud, identity theft, or other crimes. Sensitive data may be compromised without extra protection, such as encryption at rest or in transit, and requires special precautions when exchanged with the browser. XML External Entities Many older or poorly configured XML processors evaluate external entity references within XML documents. External entities can be used to disclose internal files using the file URI handler, internal file shares, internal port scanning, remote code execution, and denial of service attacks. Broken Access Control Restrictions on what authenticated users are allowed to do are often not properly enforced. Attackers can exploit these flaws to access unauthorized functionality and/or data, such as access other users' accounts, view sensitive files, modify other users' data, change access rights, etc. Security Misconfiguration Security misconfiguration is the most commonly seen issue. This is commonly a result of insecure default configurations, incomplete or ad hoc configurations, open cloud storage, misconfigured HTTP headers, and verbose error messages containing sensitive information. Not only must all operating systems, frameworks, libraries, and applications be securely configured, but they must be patched/upgraded in a timely fashion. Cross Site Scripting XSS XSS flaws occur whenever an application includes untrusted data in a new web page without proper validation or escaping, or updates an existing web page with user-supplied data using a browser API that can create HTML or JavaScript. XSS allows attackers to execute scripts in the victim's browser which can hijack user sessions, deface web sites, or redirect the user to malicious sites. Insecure deserialization Insecure deserialization often leads to remote code execution. Even if deserialization flaws do not result in remote code execution, they can be used to perform attacks, including replay attacks, injection attacks, and privilege escalation attacks. Using Components with Known Vulnerabilities Components, such as libraries, frameworks, and other software modules, run with the same privileges as the application. If a vulnerable component is exploited, such an attack can facilitate serious data loss or server takeover. Applications and APIs using components with known vulnerabilities may undermine application defenses and enable various attacks and impacts. Insufficient Logging & Monitoring Insufficient logging and monitoring, coupled with missing or ineffective integration with incident response, allows attackers to further attack systems, maintain persistence, pivot to more systems, and tamper, extract, or destroy data. Most breach studies show time to detect a breach is over 200 days, typically detected by external parties rather than internal processes or monitoring. Source Source Author Link OWASP Top Ten OWASP Fundation Link","title":"OWASP Top Ten"},{"location":"7.Security/8.OWASP.html#owasp-top-ten","text":"","title":"OWASP Top Ten"},{"location":"7.Security/8.OWASP.html#use-case","text":"Security shouldn't be an aftertough, like good habits, security best practices and recommendations should be followed.","title":"Use Case"},{"location":"7.Security/8.OWASP.html#recommendations","text":"Information The following descriptions are coming from the official OWASP Top Ten recommendations. Recommendation Description Injections Injection flaws, such as SQL, NoSQL, OS, and LDAP injection, occur when untrusted data is sent to an interpreter as part of a command or query. The attacker's hostile data can trick the interpreter into executing unintended commands or accessing data without proper authorization. Broken Authentication Application functions related to authentication and session management are often implemented incorrectly, allowing attackers to compromise passwords, keys, or session tokens, or to exploit other implementation flaws to assume other users' identities temporarily or permanently. Sensitive Data Exposure Many web applications and APIs do not properly protect sensitive data, such as financial, healthcare, and PII. Attackers may steal or modify such weakly protected data to conduct credit card fraud, identity theft, or other crimes. Sensitive data may be compromised without extra protection, such as encryption at rest or in transit, and requires special precautions when exchanged with the browser. XML External Entities Many older or poorly configured XML processors evaluate external entity references within XML documents. External entities can be used to disclose internal files using the file URI handler, internal file shares, internal port scanning, remote code execution, and denial of service attacks. Broken Access Control Restrictions on what authenticated users are allowed to do are often not properly enforced. Attackers can exploit these flaws to access unauthorized functionality and/or data, such as access other users' accounts, view sensitive files, modify other users' data, change access rights, etc. Security Misconfiguration Security misconfiguration is the most commonly seen issue. This is commonly a result of insecure default configurations, incomplete or ad hoc configurations, open cloud storage, misconfigured HTTP headers, and verbose error messages containing sensitive information. Not only must all operating systems, frameworks, libraries, and applications be securely configured, but they must be patched/upgraded in a timely fashion. Cross Site Scripting XSS XSS flaws occur whenever an application includes untrusted data in a new web page without proper validation or escaping, or updates an existing web page with user-supplied data using a browser API that can create HTML or JavaScript. XSS allows attackers to execute scripts in the victim's browser which can hijack user sessions, deface web sites, or redirect the user to malicious sites. Insecure deserialization Insecure deserialization often leads to remote code execution. Even if deserialization flaws do not result in remote code execution, they can be used to perform attacks, including replay attacks, injection attacks, and privilege escalation attacks. Using Components with Known Vulnerabilities Components, such as libraries, frameworks, and other software modules, run with the same privileges as the application. If a vulnerable component is exploited, such an attack can facilitate serious data loss or server takeover. Applications and APIs using components with known vulnerabilities may undermine application defenses and enable various attacks and impacts. Insufficient Logging & Monitoring Insufficient logging and monitoring, coupled with missing or ineffective integration with incident response, allows attackers to further attack systems, maintain persistence, pivot to more systems, and tamper, extract, or destroy data. Most breach studies show time to detect a breach is over 200 days, typically detected by external parties rather than internal processes or monitoring.","title":"Recommendations"},{"location":"7.Security/8.OWASP.html#source","text":"Source Author Link OWASP Top Ten OWASP Fundation Link","title":"Source"},{"location":"8.Road%20Map/1.client.html","text":"Client Feature Idea Apple like product presentation. Add Progressive Web Application capabilities to the site. Add fallback to WebP images with the Intersection Observer API.","title":"Client"},{"location":"8.Road%20Map/1.client.html#client","text":"","title":"Client"},{"location":"8.Road%20Map/1.client.html#feature-idea","text":"Apple like product presentation. Add Progressive Web Application capabilities to the site. Add fallback to WebP images with the Intersection Observer API.","title":"Feature Idea"},{"location":"8.Road%20Map/2.backend.html","text":"Backend Feature Idea Add CI/CD pipeline. Add/Replace MongoDB with Neo4j and build a recommendation system. Add cross selling capabilities to the store. Replace authorization middleware with an acces list middleware. Enhance search experience with autocomplete feature, maybe implement Solr or Elasticsearch. Implement a chatbot with BotPress / Olivia / Hubot , it will help increase conversion. Convert the MVC architecture into microservices with the help of Docker, Kubernetes and Istio.","title":"Backend"},{"location":"8.Road%20Map/2.backend.html#backend","text":"","title":"Backend"},{"location":"8.Road%20Map/2.backend.html#feature-idea","text":"Add CI/CD pipeline. Add/Replace MongoDB with Neo4j and build a recommendation system. Add cross selling capabilities to the store. Replace authorization middleware with an acces list middleware. Enhance search experience with autocomplete feature, maybe implement Solr or Elasticsearch. Implement a chatbot with BotPress / Olivia / Hubot , it will help increase conversion. Convert the MVC architecture into microservices with the help of Docker, Kubernetes and Istio.","title":"Feature Idea"},{"location":"a.Annexes/1.node.html","text":"Node.js Javascript Runtime Node.js is a Javascript runtime environment and not a programming language. It'uses Chrome's open source Javascript engine, V8. By incorporating V8, it has access to the OS APIs. Since Javascript is single threated, is uses an Event Loop, an Event Queue and a thread Pool for executing asychronous tasks. Node.js is highly performant for I/O tasks, but struggles with CPU instensive tasks, like file uploads. Personal Diagram, Inspired From Medium Express Middlewares Express is a light server built on top of Node.js, it comes with a lot of functionnalities and can be set really quickly.One important feature of Express, is that it is middleware based. Think of a middleware like a function that executes before a controller in a MVC architecture. Doing so, it is possible for example, to authenticate and authorize users and add business Logic in middlewares. In Express, any function which accepts requests and send a next() function back, is considered as a valid middleware. Personal Diagram Other Solutions Deno The server could also have been build on Deno, with the Oak framework, but as for the 24 March 2021, Deno is still in version 1.8, for production use, it is recommended to wait a bit more. Deno was created by the same author as Node.js, Ryan Dahl.Concerning Node.js, he had some regrets that he wanted to correct with Deno, like promises, the package manager, security, ... For this project, Deno wasn't choosed because of the learning curve due to the short deadline and the project's maturity. Golang An other solution would have been to use Golang. Golang, also called Go, is a programming language designed in Google by Robert Griesemer, Rob Pike and Ken Thompson. Go was created at a time, in 2006-2007, when multi-core processors first came out. The authors wanted to move away from single threated architectures and implement concurrency. Concurrency is the process of managing a lot of things at the same time, compared to parallelism, which is the process of doing a lot of things at the same time. Go is almost has fast as C++, it uses channels instead of threads, it implements concurrency, there is no need to worry about memory since it comes with a garbage collector and it's easy to learn. For this project, Golang wasn't choosed because of the learning curve due to the short deadline. Languages Language URI Description Typescript Link Typescript is a superset of Javascript, it allows to add types in development and detect compilation errors at compile time. Deno Link Golang Link","title":"Node.js"},{"location":"a.Annexes/1.node.html#nodejs","text":"","title":"Node.js"},{"location":"a.Annexes/1.node.html#javascript-runtime","text":"Node.js is a Javascript runtime environment and not a programming language. It'uses Chrome's open source Javascript engine, V8. By incorporating V8, it has access to the OS APIs. Since Javascript is single threated, is uses an Event Loop, an Event Queue and a thread Pool for executing asychronous tasks. Node.js is highly performant for I/O tasks, but struggles with CPU instensive tasks, like file uploads. Personal Diagram, Inspired From Medium","title":"Javascript Runtime"},{"location":"a.Annexes/1.node.html#express-middlewares","text":"Express is a light server built on top of Node.js, it comes with a lot of functionnalities and can be set really quickly.One important feature of Express, is that it is middleware based. Think of a middleware like a function that executes before a controller in a MVC architecture. Doing so, it is possible for example, to authenticate and authorize users and add business Logic in middlewares. In Express, any function which accepts requests and send a next() function back, is considered as a valid middleware. Personal Diagram","title":"Express Middlewares"},{"location":"a.Annexes/1.node.html#other-solutions","text":"","title":"Other Solutions"},{"location":"a.Annexes/1.node.html#deno","text":"The server could also have been build on Deno, with the Oak framework, but as for the 24 March 2021, Deno is still in version 1.8, for production use, it is recommended to wait a bit more. Deno was created by the same author as Node.js, Ryan Dahl.Concerning Node.js, he had some regrets that he wanted to correct with Deno, like promises, the package manager, security, ... For this project, Deno wasn't choosed because of the learning curve due to the short deadline and the project's maturity.","title":"Deno"},{"location":"a.Annexes/1.node.html#golang","text":"An other solution would have been to use Golang. Golang, also called Go, is a programming language designed in Google by Robert Griesemer, Rob Pike and Ken Thompson. Go was created at a time, in 2006-2007, when multi-core processors first came out. The authors wanted to move away from single threated architectures and implement concurrency. Concurrency is the process of managing a lot of things at the same time, compared to parallelism, which is the process of doing a lot of things at the same time. Go is almost has fast as C++, it uses channels instead of threads, it implements concurrency, there is no need to worry about memory since it comes with a garbage collector and it's easy to learn. For this project, Golang wasn't choosed because of the learning curve due to the short deadline.","title":"Golang"},{"location":"a.Annexes/1.node.html#languages","text":"Language URI Description Typescript Link Typescript is a superset of Javascript, it allows to add types in development and detect compilation errors at compile time. Deno Link Golang Link","title":"Languages"}]}